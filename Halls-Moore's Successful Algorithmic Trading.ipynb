{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Successful Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithmic backtesting requires knowledge of many areas, including psychology, mathematics,\n",
    "statistics, software development and market/exchange microstructure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting Biases\n",
    "Unfortunately,these biases have a tendency to inflate the performance rather than detract from it.\n",
    "\n",
    "1. Optimisation Bias (curve fitting or data-snooping bias)\n",
    "   \n",
    "   Optimisation bias can be minimised by keeping the number of parameters to a minimum and increasing the quantity of data points in the training set. In fact, one must also be careful of the latter as older training points can be subject to a prior regime (such as a regulatory environment) and thus may not be relevant to your current strategy.One method to help mitigate this bias is to perform a sensitivity analysis. This means varying the parameters incrementally and plotting a \"surface\" of performance. Sound, fundamental reasoning for parameter choices should, with all other factors considered, lead to a smoother parameter surface. If you have a very jumpy performance surface, it often means that a parameter is not reflecting a phenomena and is an artefact of the test data. There is a vast literature on multi-dimensional optimisation algorithms and it is a highly active area of research.\n",
    "   \n",
    "   \n",
    "2. Look-Ahead Bias\n",
    "\n",
    "    Look-ahead bias is introduced into a backtesting system when future data is accidentally included at a point in the simulation where that data would not have actually been available. Technical Bugs: Arrays/vectors in code often have iterators or index variables. Incorrect offsets of these indices can lead to a look-ahead bias by incorporating data at N + k for non-zero k. Parameter Calculation: Another common example of look-ahead bias occurs when calculating optimal strategy parameters, such as with linear regressions between two time series. If the whole data set (including future data) is used to calculate the regression coefficients, and thus retroactively applied to a trading strategy for optimisation purposes, then future data is being incorporated and a look-ahead bias exists. Maxima/Minima: since these maximal/minimal values can only be calculated at the end of a time period, a look-ahead bias is introduced if these values are used -during- the current period. It is always necessary to lag high/low values by at least one period in any trading strategy making use of them.\n",
    "    \n",
    "    \n",
    "3. Survivorship Bias\n",
    "\n",
    "    It occurs when strategies are tested on datasets that do not include the full universe of prior assets that may have been chosen at a particular point in time, but only consider those that have \"survived\" to the current time. There are two main ways to mitigate survivorship bias in your strategy backtests:Survivorship Bias Free Datasets: In the case of equity data it is possible to purchase datasets that include delisted entities, although they are not cheap and only tend to be utilised by institutional firms. Use More Recent Data: In the case of equities, utilising a more recent data set mitigates the possibility that the stock selection chosen is weighted to \"survivors\", simply as there is less likelihood of overall stock delisting in shorter time periods. \n",
    "\n",
    "\n",
    "4. Cognitive Bias\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Automated Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a strategy has been deemed suitable on a research basis it must be tested in a more realistic fashion. The ideal situation is to be able to use the same trade generation code for historical backtesting as well as live execution. This can be achieved using an event-driven backtester. Event-driven systems are widely used in software engineering, commonly for handling graphical user interface (GUI) input within window-based operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Sourcing Strategy Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    • MATLAB Trading - http://matlab-trading.blogspot.co.uk/\n",
    "    • Quantitative Trading (Ernest Chan) - http://epchan.blogspot.com\n",
    "    • Quantivity - http://quantivity.wordpress.com\n",
    "    • Quantopian - http://blog.quantopian.com\n",
    "    • Quantpedia - http://quantpedia.com\n",
    "    • Quantocracy - http://www.quantocracy.com\n",
    "    • Quant News - http://www.quantnews.com\n",
    "    • Algo Trading Sub-Reddit - http://www.reddit.com/r/algotrading\n",
    "    • Elite Trader Forums - http://www.elitetrader.com\n",
    "    • Nuclear Phynance - http://www.nuclearphynance.com\n",
    "    • QuantNet - http://www.quantnet.com\n",
    "    • Wealth Lab - http://www.wealth-lab.com/Forum\n",
    "    • Wilmott Forums - http://www.wilmott.com\n",
    "    • arXiv - http://arxiv.org/archive/q-fin\n",
    "    • SSRN - http://www.ssrn.com\n",
    "    • Journal of Investment Strategies - http://www.risk.net/type/journal/source/journalof-investment-strategies\n",
    "    • Journal of Computational Finance - http://www.risk.net/type/journal/source/journalof-computational-finance\n",
    "    • Mathematical Finance - http://onlinelibrary.wiley.com/journal/10.1111/%28ISSN%291467-9965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Financial Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full trading system: alpha model + risk management + portfolio construction\n",
    "\n",
    "A securities master is an organisation-wide database that stores fundamental, pricing and transactional data for a variety of financial instruments across asset classes.\n",
    "\n",
    "Some of the instruments that might be of interest:\n",
    "\n",
    "    Equities\n",
    "    Equity Options\n",
    "    Indices\n",
    "    Foreign Exchange\n",
    "    Interest Rates\n",
    "    Futures\n",
    "    Commodities\n",
    "    Bonds - Government and Corporate\n",
    "    Derivatives - Caps, Floors, Swaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical data structure\n",
    "For an equities master database I foresee the following entities:\n",
    "1.  Exchanges - What is the ultimate original source of the data?\n",
    "2.  Vendor - Where is a particular data point obtained from?\n",
    "3. Instrument/Ticker - The ticker/symbol for the equity or index, along with corporate information of the underlying firm or fund.\n",
    "4. Price - The actual price for a particular security on a particular day.\n",
    "5. Corporate Actions - The list of all stock splits or dividend adjustments (this may lead to one or more tables), necessary for adjusting the pricing data.\n",
    "6. National Holidays - To avoid mis-classifying trading holidays as missing data errors, it can be useful to store national holidays and cross-reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of writing software scripts to carry out the download, storage and cleaning of the\n",
    "data is that scripts can be automated via tools provided by the operating system. In UNIX-based\n",
    "systems (such as Mac OSX or Linux), one can make use of the crontab, which is a continually\n",
    "running process that allows specific scripts to be executed at custom-defined times or regular\n",
    "periods. There is an equivalent process on MS Windows known as the Task Scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Processing Financial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamental Data:\n",
    "\n",
    "    macroeconomic information such as economic growth\n",
    "    corporate earnings histories \n",
    "    inflation indexes such as CPI\n",
    "    payroll reports\n",
    "    interest rates\n",
    "    SEC filings\n",
    "    hedge fund performance reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  High         Low        Open       Close       Volume  \\\n",
      "Date                                                                      \n",
      "2015-06-09  209.100006  207.690002  208.449997  208.449997  105034700.0   \n",
      "2015-06-10  211.410004  209.300003  209.369995  210.949997  134551300.0   \n",
      "2015-06-11  212.089996  211.199997  211.479996  211.630005   73876400.0   \n",
      "2015-06-12  211.479996  209.679993  210.639999  210.009995  135382400.0   \n",
      "2015-06-15  209.449997  207.789993  208.639999  209.110001  124384200.0   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "2015-06-09  190.672104  \n",
      "2015-06-10  192.958862  \n",
      "2015-06-11  193.580902  \n",
      "2015-06-12  192.099045  \n",
      "2015-06-15  191.275818  \n"
     ]
    }
   ],
   "source": [
    "# Yahoo finance\n",
    "from __future__ import print_function\n",
    "import datetime\n",
    "from pandas_datareader import data\n",
    "if __name__ == \"__main__\":\n",
    "    spy = data.DataReader(\n",
    "        \"SPY\", \"yahoo\",\n",
    "        datetime.datetime(2007,1,1),\n",
    "        datetime.datetime(2015,6,15)\n",
    "    )\n",
    "    print(spy.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quandl data\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def construct_futures_symbols(symbol, start_year=2014, end_year=2018):\n",
    "    \"\"\"\n",
    "    Constructs a list of futures contract codes\n",
    "    for a particular symbol and timeframe.\n",
    "    \"\"\"\n",
    "    futures = []\n",
    "    # March, June, September and\n",
    "    # December delivery codes\n",
    "    months = \"HMUZ\"\n",
    "    for y in range(start_year, end_year+1):\n",
    "        for m in months:\n",
    "            futures.append(\"%s%s%s\" % (symbol, m, y))\n",
    "    return futures\n",
    "\n",
    "def download_contract_from_quandl(contract, dl_dir):\n",
    "    \"\"\"\n",
    "    Download an individual futures contract from Quandl and then\n",
    "    store it to disk in the ’dl_dir’ directory. An auth_token is\n",
    "    required, which is obtained from the Quandl upon sign-up.\n",
    "    \"\"\"\n",
    "    # Construct the API call from the contract and auth_token\n",
    "    api_call = \"http://www.quandl.com/api/v1/datasets/\"\n",
    "    api_call += \"OFDP/FUTURE_%s.csv\" % contract\n",
    "    # If you wish to add an auth token for more downloads, simply\n",
    "    # comment the following line and replace MY_AUTH_TOKEN with\n",
    "    # your auth token in the line below\n",
    "    params = \"?sort_order=asc\"\n",
    "    #params = \"?auth_token=MY_AUTH_TOKEN&sort_order=asc\"\n",
    "    full_url = \"%s%s\" % (api_call, params)\n",
    "    # Download the data from Quandl\n",
    "    data = requests.get(full_url).text\n",
    "    # Store the data to disk\n",
    "    fc = open(\"%s/%s.csv\" % (dl_dir, contract), 'w')\n",
    "    fc.write(data)\n",
    "    fc.close()\n",
    "\n",
    "def download_historical_contracts(symbol, dl_dir, start_year=2010, end_year=2014):\n",
    "    \"\"\"\n",
    "    Downloads all futures contracts for a specified symbol\n",
    "    between a start_year and an end_year.\n",
    "    \"\"\"\n",
    "    contracts = construct_futures_symbols(\n",
    "    symbol, start_year, end_year\n",
    "    )\n",
    "    for c in contracts:\n",
    "        print(\"Downloading contract: %s\" % c)\n",
    "        download_contract_from_quandl(c, dl_dir)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    symbol = 'ES'\n",
    "    # Make sure you’ve created this\n",
    "    # relative directory beforehand\n",
    "    dl_dir = 'Q:/ES'\n",
    "    # Create the start and end years\n",
    "    start_year = 2014\n",
    "    end_year = 2018\n",
    "    # Download the contracts into the directory\n",
    "    download_historical_contracts(\n",
    "        symbol, dl_dir, start_year, end_year\n",
    "    )\n",
    "    # Open up a single contract via read_csv\n",
    "    # and plot the settle price\n",
    "    es = pd.io.parsers.read_csv(\n",
    "        \"%s/ESH2014.csv\" % dl_dir, index_col=\"Date\"\n",
    "    )\n",
    "    es[\"Settle\"].plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous futures\n",
    "from __future__ import print_function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Quandl\n",
    "\n",
    "def futures_rollover_weights(start_date, expiry_dates,\n",
    "    contracts, rollover_days=5):\n",
    "    \"\"\"This constructs a pandas DataFrame that contains weights\n",
    "    (between 0.0 and 1.0) of contract positions to hold in order to\n",
    "    carry out a rollover of rollover_days prior to the expiration of\n",
    "    the earliest contract. The matrix can then be ’multiplied’ with\n",
    "    another DataFrame containing the settle prices of each\n",
    "    contract in order to produce a continuous time series\n",
    "    futures contract.\"\"\"\n",
    "    # Construct a sequence of dates beginning\n",
    "    # from the earliest contract start date to the end\n",
    "    # date of the final contract\n",
    "    dates = pd.date_range(start_date, expiry_dates[-1], freq=’B’)\n",
    "    # Create the ’roll weights’ DataFrame that will store the multipliers for\n",
    "    # each contract (between 0.0 and 1.0)\n",
    "    roll_weights = pd.DataFrame(np.zeros((len(dates), len(contracts))),\n",
    "    index=dates, columns=contracts)\n",
    "    prev_date = roll_weights.index[0]\n",
    "    # Loop through each contract and create the specific weightings for\n",
    "    # each contract depending upon the settlement date and rollover_days\n",
    "    for i, (item, ex_date) in enumerate(expiry_dates.iteritems()):\n",
    "        if i < len(expiry_dates) - 1:\n",
    "            roll_weights.ix[prev_date:ex_date - pd.offsets.BDay(), item] = 1\n",
    "            roll_rng = pd.date_range(end=ex_date - pd.offsets.BDay(),periods=rollover_days + 1, freq=’B’)\n",
    "            # Create a sequence of roll weights (i.e. [0.0,0.2,...,0.8,1.0]\n",
    "            # and use these to adjust the weightings of each future\n",
    "            decay_weights = np.linspace(0, 1, rollover_days + 1)\n",
    "            roll_weights.ix[roll_rng, item] = 1 - decay_weights\n",
    "            roll_weights.ix[roll_rng, expiry_dates.index[i+1]] = decay_weights\n",
    "        else:\n",
    "            roll_weights.ix[prev_date:, item] = 1\n",
    "    prev_date = ex_date\n",
    "    return roll_weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Download the current Front and Back (near and far) futures contracts\n",
    "    # for WTI Crude, traded on NYMEX, from Quandl.com. You will need to\n",
    "    # adjust the contracts to reflect your current near/far contracts\n",
    "    # depending upon the point at which you read this!\n",
    "    wti_near = Quandl.get(\"OFDP/FUTURE_CLF2014\")\n",
    "    wti_far = Quandl.get(\"OFDP/FUTURE_CLG2014\")\n",
    "    wti = pd.DataFrame({’CLF2014’: wti_near[’Settle’],’CLG2014’: wti_far[’Settle’]}, index=wti_far.index)\n",
    "    # Create the dictionary of expiry dates for each contract\n",
    "    expiry_dates = pd.Series(\n",
    "        {’CLF2014’: datetime.datetime(2013, 12, 19),\n",
    "        ’CLG2014’: datetime.datetime(2014, 2, 21)}).order()\n",
    "    # Obtain the rollover weighting matrix/DataFrame\n",
    "    weights = futures_rollover_weights(wti_near.index[0],expiry_dates, wti.columns)\n",
    "    # Construct the continuous future of the WTI CL contracts\n",
    "    wti_cts = (wti * weights).sum(1).dropna()\n",
    "    # Output the merged series of contract settle prices\n",
    "    print(wti_cts.tail(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the modelling is to provide a robust quantitative framework for identifying relationships in financial market data that can be exploited to generate profitable trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Statistical Learning?\n",
    "\n",
    "In a more quantitative sense we are attempting to model the behaviour of an outcome or response based on a set of predictors or features assuming a relationship between the two.\n",
    "\n",
    "This can be formalised by considering a response Y with p different features x1, x2, ..., xp. If\n",
    "we utilise vector notation then we can define X = (x1, x2, ..., xp), which is a vector of length p.\n",
    "    \n",
    "    Y = f(X) + e\n",
    "    \n",
    "e represents error or noise term. This term is included to represent information that is not considered within f.\n",
    "\n",
    "The goal of statistical learning is to estimate the form of f based on the observed data and to evaluate how accurate those estimates are.\n",
    "\n",
    "There are two general tasks that are of interest in statistical learning: prediction and inference.\n",
    "    \n",
    "\n",
    "\"Prediction\" is concerned with predicting a response Y based on a newly observed predictor, X. If the model relationship has been determined then it is simple to predict the response using an estimate for f to produce an estimate for the response. The functional form of f is often unimportant in a prediction scenario assuming that the estimated responses are close to the true responses and is thus accurate in its predictions. Different estimates of f will produce various accuracies of the estimates of Y . The error associated with having a poor estimate of f is called the \"reducible error\". Note that there is always a degree of \"irreducible error\" because our original specification of the problem included the error term. This error term encapsulates the unmeasured factors that may affect the response Y . The approach taken is to try and minimise the reducible error with the understanding that there will always be an upper limit of accuracy based on the irreducible error.\n",
    "\n",
    "\"Inference\" is concerned with the situation where there is a need to understand the relationship between X and Y and hence its exact form must be determined. One may wish to identify important predictors or determine the relationship between individual predictors and the response. One could also ask if the relationship is linear or non-linear. The former means the model is likely to be more interpretable but at the expense of potentially worse predictability. The latter provides models which are generally more predictive but are sometimes less interpretable. Hence a trade-off between predictability and interpretability often exists.\n",
    "\n",
    "\"Parametric Models\": The defining feature of parametric methods is that they require the specification or assumption\n",
    "of the form of f. This is a modelling decision. The first choice is whether to consider a linear or non-linear model. Why p + 1 and not p? Since linear models can be affine, that is they may not pass through\n",
    "the origin when creating a \"line of best fit\", a coefficient is required to specify the \"intercept\". Now that we have specified a (linear) functional form of f we need to train it. \"Training\" in this instance means finding an estimate for β.In the linear setting we can use an algorithm such as ordinary least squares (OLS) but other\n",
    "methods are available as well. This can lead to poor estimates because the model is not flexible enough. A potential remedy is to consider adding more parameters, by choosing alternate forms for f^. Unfortunately if the model becomes too flexible it can lead to a very dangerous situation known as overfitting. In essence, the model follows the noise too closely and not the signal.\n",
    "\n",
    "\"Non-Parametric Models\": The alternative approach is to consider a non-parametric form of f^. The benefit is that it can potentially fit a wider range of possible forms for f and is thus more flexible. Unfortunately\n",
    "non-parametric models suffer from the need to have an extensive amount of observational data\n",
    "points, often far more than in a parametric settings. In addition non-parametric methods are\n",
    "also prone to overfitting if not treated carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques\n",
    "\n",
    "1. Regression: Regression refers to a broad group of supervised machine learning techniques that provide both predictive and inferential capabilities.Regression tries to model the relationship between a dependent variable (response) and a set of independent variables (predictors). In particular, the goal of regression is to ascertain the change in a response, when one of the independent variables changes, under the assumption that the remaining independent variables are kept fixed. It is including Linear regression and logistic regression.\n",
    "2. Classification: Classifiers can be utilised in algorithmic trading to predict whether a particular time series will have positive or negative returns in subsequent (unknown) time periods. This is similar to a regression setting except that the actual value of the time series is not being predicted, rather its direction. It is including Logistic Regression, Linear/Quadratic Discriminant Analysis, Support Vector Machines (SVM) and Artificial Neural Networks (ANN).\n",
    "3. Time Series Models: There are two broad families of time series models that interest us in algorithmic trading. The first set are the linear autoregressive integrated moving average (ARIMA) family of models, which are used to model the variations in the absolute value of a time series. The other family of time series are the autoregressive conditional heteroskedasticity (ARCH) models, which are used to model the variance (i.e. the volatility) of time series over time. ARCH models use previous values (volatilities) of the time series to predict future values (volatilities). This is in contrast to stochastic volatility models, which utilise more than one stochastic time series (i.e. multiple stochastic differential equations) to model volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Time Series Analysis\n",
    "\n",
    "In this chapter we are going to consider statistical tests that will help us identify price series that\n",
    "possess trending or mean-reverting behaviour. If we can identify such series statistically then we\n",
    "can capitalise on this behaviour by forming momentum or mean-reverting trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Mean Reversion\n",
    "\n",
    "Mean-reverting strategies form a large component of the statistical arbitrage quant hedge funds. The basic idea when trying to ascertain if a time series is mean-reverting is to use a statistical test to see if it differs from the behaviour of a random walk - the time series has no \"memory\" of where it has been. A mean-reverting time series, however, is different. The change in the value of the time series in the next time period is proportional to the current value. Specifically, it is proportional to the difference between the mean historical price and the current price. Mathematically, such a (continuous) time series is referred to as an Ornstein-Uhlenbeck process. If we can show, statistically, that a price series behaves like an Ornstein-Uhlenbeck series then we can begin the process of forming a trading strategy around it. As stated above, a continuous mean-reverting time series can be represented by an Ornstein-Uhlenbeck stochastic differential equation:\n",
    "    \n",
    "    dxt = θ(µ − xt)dt + σdWt\n",
    "    \n",
    "Where θ is the rate of reversion to the mean, µ is the mean value of the process, σ is the variance of the process and Wt is a Wiener Process or Brownian Motion. This equation essentially states that the change of the price series in the next continuous time period is proportional to the difference between the mean price and the current price, with the\n",
    "addition of Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented Dickey-Fuller (ADF) Test\n",
    "The ADF test makes use of the fact that if a price series possesses mean reversion, then the next price level will be proportional to the current price level. Mathematically, the ADF is based on the idea of testing for the presence of a unit root in an autoregressive time series sample. We can consider a model for a time series, known as a linear lag model of order p. This model says that the change in the value of the time series is proportional to a constant, the time itself and the previous p values of the time series, along with an error term:\n",
    "\n",
    "    ∆yt = α + βt + γyt−1 + δ1∆yt−1 + · · · + δp−1∆yt−p+1 + et\n",
    "    \n",
    "The role of the ADF hypothesis test is to ascertain, statistically, whether γ = 0, which would indicate (with α = β = 0) that the process is a random walk and thus non mean reverting. Hence we are testing for the null hypothesis that γ = 0. If the hypothesis that γ = 0 can be rejected then the following movement of the price series\n",
    "is proportional to the current price and thus it is unlikely to be a random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-7a713d88b0bf>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-7a713d88b0bf>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    ts.adfuller(amzn[’Adj Close’], 1)\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# We will carry out the ADF test on a sample price series of Amazon stock, from 1st January 2000 to 1st January 2015.\n",
    "from __future__ import print_function\n",
    "# Import the Time Series library\n",
    "import statsmodels.tsa.stattools as ts\n",
    "# Import Datetime and the Pandas DataReader\n",
    "from datetime import datetime\n",
    "import pandas.io.data as web\n",
    "# Download the Amazon OHLCV data from 1/1/2000 to 1/1/2015\n",
    "amzn = web.DataReader(\"AMZN\", \"yahoo\", datetime(2000,1,1), datetime(2015,1,1))\n",
    "# Output the results of the Augmented Dickey-Fuller test for Amazon\n",
    "# with a lag order value of 1\n",
    "ts.adfuller(amzn[’Adj Close’], 1)\n",
    "\n",
    "# The first value is the calculated test-statistic, while the second value is the p-value. \n",
    "# The fourth is the number of data points in the sample. \n",
    "# The fifth value, the dictionary, contains the critical values of the test-statistic at the 1, 5 and 10 percent values respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Since the calculated value of the test statistic is larger than any of the critical values at the 1,\n",
    "5 or 10 percent levels, we cannot reject the null hypothesis of γ = 0 and thus we are unlikely to\n",
    "have found a mean reverting time series. This is in line with our tuition as most equities behave\n",
    "akin to Geometric Brownian Motion (GBM), i.e. a random walk\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Stationarity\n",
    "\n",
    "A time series (or stochastic process) is defined to be strongly stationary if its joint probability\n",
    "distribution is invariant under translations in time or space. In particular, and of key importance\n",
    "for traders, the mean and variance of the process do not change over time or space and they each\n",
    "do not follow a trend. A critical feature of stationary price series is that the prices within the series diffuse from their initial value at a rate slower than that of a GBM. By measuring the rate of this diffusive behaviour\n",
    "we can identify the nature of the time series and thus detect whether it is mean-reverting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hurst Exponent\n",
    "\n",
    "The goal of the Hurst Exponent is to provide us with a scalar value that will help us to identify\n",
    "(within the limits of statistical estimation) whether a series is mean reverting, random walking\n",
    "or trending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from numpy import cumsum, log, polyfit, sqrt, std, subtract\n",
    "from numpy.random import randn\n",
    "\n",
    "def hurst(ts):\n",
    "    \"\"\"Returns the Hurst Exponent of the time series vector ts\"\"\"\n",
    "    # Create the range of lag values\n",
    "    lags = range(2, 100)\n",
    "    # Calculate the array of the variances of the lagged differences\n",
    "    tau = [sqrt(std(subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "    # Use a linear fit to estimate the Hurst Exponent\n",
    "    poly = polyfit(log(lags), log(tau), 1)\n",
    "    # Return the Hurst exponent from the polyfit output\n",
    "    return poly[0]*2.0\n",
    "\n",
    "# Create a Gometric Brownian Motion, Mean-Reverting and Trending Series\n",
    "gbm = log(cumsum(randn(100000))+1000)\n",
    "mr = log(randn(100000)+1000)\n",
    "tr = log(cumsum(randn(100000)+1)+1000)\n",
    "\n",
    "# Output the Hurst Exponent for each of the above series\n",
    "# and the price of Amazon (the Adjusted Close price) for\n",
    "# the ADF test given above in the article\n",
    "print(\"Hurst(GBM): %s\" % hurst(gbm))\n",
    "print(\"Hurst(MR): %s\" % hurst(mr))\n",
    "print(\"Hurst(TR): %s\" % hurst(tr))\n",
    "# Assuming you have run the above code to obtain ’amzn’!\n",
    "print(\"Hurst(AMZN): %s\" % hurst(amzn[’Adj Close’]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cointergration\n",
    "\n",
    "It is actually very difficult to find a tradable asset that possesses mean-reverting behaviour. Equities broadly behave like GBMs and hence render the mean-reverting trade strategies relatively\n",
    "useless. However, there is nothing stopping us from creating a portfolio of price series that is\n",
    "stationary. Hence we can apply mean-reverting trading strategies to the portfolio. The simplest form of mean-reverting trade strategies is the classic \"pairs trade\", which usually involves a dollar-neutral long-short pair of equities. The theory goes that two companies in the same sector are likely to be exposed to similar market factors, which affect their businesses. Occasionally their relative stock prices will diverge due to certain events, but will revert to the\n",
    "long-running mean. The pairs trade essentially works by using a linear model for a relationship between the two\n",
    "stock prices (Beta is the hedging ratio needed to form the linear combination):\n",
    "\n",
    "    y(t) = βx(t) + e(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cointegrated Augmented Dickey-Fuller Test\n",
    "\n",
    "In order to statistically confirm whether this series is mean-reverting we could use one of the tests\n",
    "we described above, namely the Augmented Dickey-Fuller Test or the Hurst Exponent. However,\n",
    "neither of these tests will actually help us determine β, they will only tell us whether, for a particular β, the linear combination isstationary.\n",
    "This is where the Cointegrated Augmented Dickey-Fuller (CADF) test comes in. It determines\n",
    "the optimal hedge ratio by performing a linear regression against the two time series and then\n",
    "tests for stationarity under the linear combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# cadf.py\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import pandas.io.data as web\n",
    "import pprint\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from pandas.stats.api import ols\n",
    "\n",
    "# cadf.py\n",
    "def plot_price_series(df, ts1, ts2):\n",
    "    months = mdates.MonthLocator() # every month\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df.index, df[ts1], label=ts1)\n",
    "    ax.plot(df.index, df[ts2], label=ts2)\n",
    "    ax.xaxis.set_major_locator(months)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(’%b %Y’))\n",
    "    ax.set_xlim(datetime.datetime(2012, 1, 1), datetime.datetime(2013, 1, 1))\n",
    "    ax.grid(True)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.xlabel(’Month/Year’)\n",
    "    plt.ylabel(’Price ($)’)\n",
    "    plt.title(’%s and %s Daily Prices’ % (ts1, ts2))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# cadf.py\n",
    "def plot_scatter_series(df, ts1, ts2):\n",
    "    plt.xlabel(’%s Price ($)’ % ts1)\n",
    "    plt.ylabel(’%s Price ($)’ % ts2)\n",
    "    plt.title(’%s and %s Price Scatterplot’ % (ts1, ts2))\n",
    "    plt.scatter(df[ts1], df[ts2])\n",
    "    plt.show()\n",
    "    \n",
    "# cadf.py\n",
    "def plot_residuals(df):\n",
    "    months = mdates.MonthLocator() # every month\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df.index, df[\"res\"], label=\"Residuals\")\n",
    "    ax.xaxis.set_major_locator(months)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(’%b %Y’))\n",
    "    ax.set_xlim(datetime.datetime(2012, 1, 1), datetime.datetime(2013, 1, 1))\n",
    "    ax.grid(True)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.xlabel(’Month/Year’)\n",
    "    plt.ylabel(’Price ($)’)\n",
    "    plt.title(’Residual Plot’)\n",
    "    plt.legend()\n",
    "    plt.plot(df[\"res\"])\n",
    "    plt.show()\n",
    "    \n",
    "# cadf.py\n",
    "if __name__ == \"__main__\":\n",
    "    start = datetime.datetime(2012, 1, 1)\n",
    "    end = datetime.datetime(2013, 1, 1)\n",
    "    arex = web.DataReader(\"AREX\", \"yahoo\", start, end)\n",
    "    wll = web.DataReader(\"WLL\", \"yahoo\", start, end)\n",
    "    df = pd.DataFrame(index=arex.index)\n",
    "    df[\"AREX\"] = arex[\"Adj Close\"]\n",
    "    df[\"WLL\"] = wll[\"Adj Close\"]\n",
    "    # Plot the two time series\n",
    "    plot_price_series(df, \"AREX\", \"WLL\")\n",
    "    # Display a scatter plot of the two time series\n",
    "    plot_scatter_series(df, \"AREX\", \"WLL\")\n",
    "    # Calculate optimal hedge ratio \"beta\"\n",
    "    res = ols(y=df[’WLL’], x=df[\"AREX\"])\n",
    "    beta_hr = res.beta.x\n",
    "    # Calculate the residuals of the linear combination\n",
    "    df[\"res\"] = df[\"WLL\"] - beta_hr*df[\"AREX\"]\n",
    "    # Plot the residuals\n",
    "    plot_residuals(df)\n",
    "    # Calculate and output the CADF test on the residuals\n",
    "    cadf = ts.adfuller(df[\"res\"])\n",
    "    pprint.pprint(cadf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Forcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Forecasting Accuracy\n",
    "\n",
    "1. Hit Rate: \"How many times did we predict the correct direction, as a percentage of all predictions?\"\n",
    "2. Confusion Matrix (or contingency table): \"How many times did we predict up correctly and how many times did we predict down correctly? Did they differ substantially?\" For instance, it might turn out that a particular algorithm is consistently more accurate at predicting \"down days\". This motivates a strategy that emphasises shorting of a financial asset to increase profitability. A confusion matrix characterises this idea by determining the false positive rate (known statistically as a Type I error) and false negative rate (known statistically as a Type II error) for a supervised classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Choice\n",
    "\n",
    "One of the most crucial aspects of asset price forecasting is choosing the factors used as predictors. Factor choice is carried out by trying to determine the fundamental drivers of asset movement.\n",
    "\n",
    "1. Lagged Price Factors and Volume: The first type of factor that is often considered in forecasting a time series are prior historical values of the time series itself. Thus a set of p factors could be easily obtained by creating p lags of the time series close price. In addition to the price series itself we can also incorporate traded volume as an indicator. Thus we can create a p + 1-dimensional feature vector for each day of the time series, which incorporates the p time lags and the volume series.\n",
    "2. External Factors:There are a vast amount of macroeconomic time series and asset prices series on which to consider forecasts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models\n",
    "\n",
    "1. Logistic Regression (LR): The logistic regression model provides the probability that a particular subsequent time period will be categorised as \"up\" or \"down\". Thus the model introduces a parameter, namely the probability threshold for classifying whether a subsequent time period is \"up\" or \"down\". To fit the model (i.e. estimate the βi coefficients) the maximum likelihood method is used. \n",
    "\n",
    "\n",
    "2. Discriminant Analysis: Discriminant analysis is an alternative statistical technique to logistic regression. While logistic regression is less restrictive in its assumptions than discriminant analysis, it can give greater predictive performance if the more restrictive assumptions are met. In logistic regression we model the probability of seeing an \"up\" time period, given the previous two lagged returns (P(Y = UjL1; L2)) as a conditional distribution of the response Y given the predictors Li, using a logistic function. In Linear Discriminant Analysis (LDA) the distribution of the Li variables are modelled separately, given Y , and P(Y = U|L1, L2) is obtained via Bayes’ Theorem. One important mathematical assumption of LDA is that all classes (e.g. \"up\" and \"down\") share the same covariance matrix. Quadratic Discriminant Analysis (QDA) is closely reed to LDA. The significant difference is that each class can now possess its own covariance matrix.\n",
    "\n",
    "\n",
    "3. Support Vector Machines (SVM): SVCs work by attempting to locate a linear separation boundary in feature space that correctly classifies most, but not all, of the training observations by creating an optimal separation boundary between the two classes. \n",
    "\n",
    "\n",
    "4. Decision Trees and Random Forests\n",
    "\n",
    "\n",
    "5. Principal Components Analysis (PCA): Common use cases for unsupervised techniques include reducing the number of dimensions of a problem to only those considered important. We are going to utilise an unsupervised technique known as Principal Components Analysis (PCA) to reduce the size of the feature space prior to use in our supervised classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Stock Index Movement\n",
    "\n",
    "In this section we are going to use a set of classifiers to predict the direction of the closing\n",
    "price at day k based solely on price information known at day k − 1. An upward directional\n",
    "move means that the closing price at k is higher than the price at k − 1, while a downward move\n",
    "implies a closing price at k lower than at k − 1.\n",
    "If we can determine the direction of movement in a manner that significantly exceeds a 50%\n",
    "hit rate, with low error and a good statistical significance, then we are on the road to forming a\n",
    "basic systematic trading strategy based on our forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from pandas_datareader import DataReader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "def create_lagged_series(symbol, start_date, end_date, lags=5):\n",
    "    \"\"\"\n",
    "    This creates a Pandas DataFrame that stores the percentage returns of the adjusted closing value of\n",
    "    a stock obtained from Yahoo Finance, along with a\n",
    "    number of lagged returns from the prior trading days\n",
    "    (lags defaults to 5 days). Trading volume, as well as\n",
    "    the Direction from the previous day, are also included.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain stock information from Yahoo Finance\n",
    "    ts = DataReader(\n",
    "        symbol, \"yahoo\",\n",
    "        start_date-datetime.timedelta(days=365),\n",
    "        end_date\n",
    "    )\n",
    "    \n",
    "    # Create the new lagged DataFrame\n",
    "    tslag = pd.DataFrame(index=ts.index)\n",
    "    tslag[\"Today\"] = ts[\"Adj Close\"]\n",
    "    tslag[\"Volume\"] = ts[\"Volume\"]\n",
    "    \n",
    "    # Create the shifted lag series of prior trading period close values\n",
    "    for i in range(0, lags):\n",
    "        tslag[\"Lag%s\" % str(i+1)] = ts[\"Adj Close\"].shift(i+1)\n",
    "    \n",
    "    # Create the returns DataFrame\n",
    "    tsret = pd.DataFrame(index=tslag.index)\n",
    "    tsret[\"Volume\"] = tslag[\"Volume\"]\n",
    "    tsret[\"Today\"] = tslag[\"Today\"].pct_change()*100.0\n",
    "    \n",
    "    # If any of the values of percentage returns equal zero, set them to\n",
    "    # a small number (stops issues with QDA model in Scikit-Learn)\n",
    "    for i,x in enumerate(tsret[\"Today\"]):\n",
    "        if (abs(x) < 0.0001):\n",
    "            tsret[\"Today\"][i] = 0.0001\n",
    "    \n",
    "    # Create the lagged percentage returns columns\n",
    "    for i in range(0, lags):\n",
    "        tsret[\"Lag%s\" % str(i+1)] = \\\n",
    "        tslag[\"Lag%s\" % str(i+1)].pct_change()*100.0\n",
    "    \n",
    "    # Create the \"Direction\" column (+1 or -1) indicating an up/down day\n",
    "    tsret[\"Direction\"] = np.sign(tsret[\"Today\"])\n",
    "    tsret = tsret[tsret.index >= start_date]\n",
    "    return tsret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\n",
    "        \"^GSPC\", datetime.datetime(2001,1,10),\n",
    "        datetime.datetime(2005,12,31), lags=5\n",
    "    )\n",
    "    \n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    \n",
    "    # The test data is split into two parts: Before and after 1st Jan 2005.\n",
    "    start_test = datetime.datetime(2005,1,1)\n",
    "    \n",
    "    # Create training and test sets\n",
    "    X_train = X[X.index < start_test]\n",
    "    X_test = X[X.index >= start_test]\n",
    "    y_train = y[y.index < start_test]\n",
    "    y_test = y[y.index >= start_test]\n",
    "    \n",
    "    # Create the (parametrised) models\n",
    "    print(\"Hit Rates/Confusion Matrices:\\n\")\n",
    "    models = [(\"LR\", LogisticRegression()),\n",
    "            (\"LDA\", LDA()),\n",
    "            (\"QDA\", QDA()),\n",
    "            (\"LSVC\", LinearSVC()),\n",
    "            (\"RSVM\", SVC(\n",
    "            C=1000000.0, cache_size=200, class_weight=None,\n",
    "            coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',\n",
    "            max_iter=-1, probability=False, random_state=None,\n",
    "            shrinking=True, tol=0.001, verbose=False)\n",
    "            ),\n",
    "            (\"RF\", RandomForestClassifier(\n",
    "            n_estimators=1000, criterion='gini',\n",
    "            max_depth=None, min_samples_split=2,\n",
    "            min_samples_leaf=1, max_features='auto',\n",
    "            bootstrap=True, oob_score=False, n_jobs=1,\n",
    "            random_state=None, verbose=0)\n",
    "            )]\n",
    "    \n",
    "    # Iterate through the models\n",
    "    for m in models:\n",
    "        # Train each of the models on the training set\n",
    "        m[1].fit(X_train, y_train)\n",
    "        # Make an array of predictions on the test set\n",
    "        pred = m[1].predict(X_test)\n",
    "        # Output the hit-rate and the confusion matrix for each model\n",
    "        print(\"%s:\\n%0.3f\" % (m[0], m[1].score(X_test, y_test)))\n",
    "        print(\"%s\\n\" % confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Performance Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is imperative that we measure the performance, at multiple levels of granularity, of how and why our system is producing these profits. This motivates performance assessment at the level of trades, strategies and portfolios. The particular items of quantitative analysis of performance that we will be interested in are as follows:\n",
    "- Returns - The most visible aspect of a trading strategy concerns the percentage gain since inception, either in a backtest or a live trading environment. The two major performance measures here are Total Return and Compound Annual Growth Rate (CAGR).\n",
    "\n",
    "\n",
    "- Drawdowns - A drawdown is a period of negative performance, as defined from a prior high-water mark, itself defined as the previous highest peak on a strategy or portfolio equity curve. We will define this more concretely below, but you can think of it for now as a (somewhat painful!) downward slope on your performance chart.\n",
    "\n",
    "\n",
    "- Risk - Risk comprises many areas, and we’ll spend significant time going over them in the following chapter, but generally it refers to both risk of capital loss, such as with drawdowns, and volatility of returns. The latter usually being calculated as an annualised standard deviation of returns.\n",
    "\n",
    "\n",
    "- Risk/Reward Ratio - Institutional investors are mainly interested with risk-adjusted returns. Since higher volatility can often lead to higher returns at the expense of greater drawdowns, they are always concerned with how much risk is being taken on per unit of return. Consequently a range of performance measures have been invented to quantify this aspect of strategy performance, namely the Sharpe Ratio, Sortino Ratio and CALMAR Ratio, among others. The out of sample Sharpe is often the first metric to be discussed by institutional investors when discussing strategy performance.\n",
    "\n",
    "\n",
    "- Trade Analysis - The previous measures of performance are all applicable to strategies and portfolios. It is also instructive to look at the performance of individual trades and many measures exist to characterise their performance. In particular, we will quantity the number of winning/losing trades, mean profit per trade and win/loss ratio among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade Analysis\n",
    "\n",
    "The performance of the actual trades can vary dramatically between strategies.Trend-following strategies usually consist of many losing trades, each with a likely small loss. The lesser quantity of profitable trades occur when a trend has been established and the performance from these positive trades can significantly exceed the losses of the larger quantity of losing trades. Pair-trading mean-reverting strategies display the opposing character. They\n",
    "generally consist of many small profitable trades. However, if a series does not mean revert in\n",
    "the manner expected then the long/short nature of the strategy can lead to substantial losses.\n",
    "This could potentially wipe out the large quantity of small gains.\n",
    "\n",
    "- Total Profit/Loss (PnL) - The total PnL straightforwardly states whether a particular\n",
    "trade has been profitable or not.\n",
    "- Average Period PnL - The avg. period PnL states whether a bar, on average, generates\n",
    "a profit or loss.\n",
    "- Maximum Period Profit - The largest bar-period profit made by this trade so far.\n",
    "- Maximum Period Loss - The largest bar-period loss made by this trade so far. Note\n",
    "that this says nothing about future maximum period loss! A future loss could be much\n",
    "larger than this.\n",
    "- Average Period Profit - The average over the trade lifetime of all profitable periods.\n",
    "- Average Period Loss - The average over the trade lifetime of all unprofitable periods.\n",
    "- Winning Periods - The count of all winning periods.\n",
    "- Losing Periods - The count of all losing periods.\n",
    "- Percentage Win/Loss Periods - The percentage of all winning periods to losing periods.\n",
    "Will differ markedly for trend-following and mean-reverting type strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy and Portfolio Analysis\n",
    "\n",
    "For higher-frequency strategies, we will be less interested in any individual trade and instead will want to consider the performance measures of the strategy instead.\n",
    "\n",
    "- Returns Analysis - The returns of a strategy encapsulate the concept of profitability. In\n",
    "institutional settings they are generally quoted net of fees and so provide a true picture\n",
    "of how much money was made on money invested. Returns can be tricky to calculate,\n",
    "especially with cash inflows/outflows.\n",
    "- Risk/Reward Analysis - Generally the first consideration that external investors will\n",
    "have in a strategy is its out of sample Sharpe Ratio (which we describe below). This is an\n",
    "industry standard metric which attempts to characterise how much return was achieved\n",
    "per unit of risk.\n",
    "- Drawdown Analysis - In an institutional setting, this is probably the most important of\n",
    "the three aspects. The profile and extent of the drawdowns of a strategy, portfolio or fund\n",
    "form a key component in risk management. We’ll define drawdowns below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Returns Analysis: The most widely quoted figures when discussing strategy performance, in both institutional and retail settings, are often total return, annual returns and monthly returns. Note that this formula is only applicable to long-only un-leveraged portfolios. If we wish to add in short selling or leverage we need to modify how we calculate returns because we are technically trading on a larger borrowed portfolio than that used here. This is known as a margin portfolio. The equity curve is often one of the most emphasised visualisations on a hedge fund performance report - assuming the fund is doing well! It is a plot of the portfolio value of the fund over time. In essence it is used to show how the account has grown since fund inception. Equally, in a retail setting it is used to show growth of account equity through time. It gives a \"flavour\" as to the past volatility of the strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Risk/Reward Analysis:\n",
    "    \n",
    "    - Sharpe Ratio: \n",
    "    - Sortino Ratio: The Sortino ratio is motivated by the fact that the Sharpe ratio captures both upward and downward volatility in its denominator. However, investors (and hedge fund managers) are generally not too bothered when we have significant upward volatility!\n",
    "    - CALMAR Ratio: One could also argue that investors/traders are concerned solely with the maximum extent of the drawdown, rather than the average drawdown. This motivates the CALMAR (CALifornia Managed Accounts Reports) ratio, also known as the Drawdown ratio, which provides a ratio of mean excess return to the maximum drawdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader as web\n",
    "\n",
    "def annualised_sharpe(returns, N=252):\n",
    "    \"\"\"\n",
    "    Calculate the annualised Sharpe ratio of a returns stream\n",
    "    based on a number of trading periods, N. N defaults to 252,\n",
    "    which then assumes a stream of daily returns.\n",
    "    The function assumes that the returns are the excess of\n",
    "    those compared to a benchmark.\n",
    "    \"\"\"\n",
    "    return np.sqrt(N) * returns.mean() / returns.std()\n",
    "\n",
    "def equity_sharpe(ticker):\n",
    "    \"\"\"\n",
    "    Calculates the annualised Sharpe ratio based on the daily\n",
    "    returns of an equity ticker symbol listed in Google Finance.\n",
    "    The dates have been hardcoded here for brevity.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime(2000,1,1)\n",
    "    end = datetime.datetime(2013,1,1)\n",
    "    # Obtain the equities daily historic data for the desired time period\n",
    "    # and add to a pandas DataFrame\n",
    "    pdf = web.DataReader(ticker, 'google', start, end)\n",
    "    # Use the percentage change method to easily calculate daily returns\n",
    "    pdf['daily_ret'] = pdf['Close'].pct_change()\n",
    "    # Assume an average annual risk-free rate over the period of 5%\n",
    "    pdf['excess_daily_ret'] = pdf['daily_ret'] - 0.05/252\n",
    "    # Return the annualised Sharpe ratio based on the excess daily returns\n",
    "    return annualised_sharpe(pdf['excess_daily_ret'])\n",
    "\n",
    "def market_neutral_sharpe(ticker, benchmark):\n",
    "    \"\"\"\n",
    "    Calculates the annualised Sharpe ratio of a market\n",
    "    neutral long/short strategy inolving the long of ’ticker’\n",
    "    with a corresponding short of the ’benchmark’.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime(2000, 1, 1)\n",
    "    end = datetime.datetime(2013, 1, 1)\n",
    "    # Get historic data for both a symbol/ticker and a benchmark ticker\n",
    "    # The dates have been hardcoded, but you can modify them as you see fit!\n",
    "    tick = web.DataReader(ticker, 'google', start, end)\n",
    "    bench = web.DataReader(benchmark, 'google', start, end)\n",
    "    # Calculate the percentage returns on each of the time series\n",
    "    tick['daily_ret'] = tick['Close'].pct_change()\n",
    "    bench['daily_ret'] = bench['Close'].pct_change()\n",
    "    # Create a new DataFrame to store the strategy information\n",
    "    # The net returns are (long - short)/2, since there is twice\n",
    "    # the trading capital for this strategy\n",
    "    strat = pd.DataFrame(index=tick.index)\n",
    "    strat['net_ret'] = (tick['daily_ret'] - bench['daily_ret'])/2.0\n",
    "    # Return the annualised Sharpe ratio for this strategy\n",
    "    return annualised_sharpe(strat['net_ret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drawdown Analysis: Drawdown analysis concerns the measurement of drops in account equity from previous high water marks. A high water mark is defined as the last account equity peak reached on the equity curve. In an institutional setting the concept of drawdown is especially important as most hedge funds are remunerated only when the account equity is continually creating new high water marks. That is, a fund manager is not paid a performance fee while the fund remains \"under water\", i.e. the account equity is in a period of drawdown.\n",
    "Most investors would be concerned at a drawdown of 10% in a fund, and would likely redeem\n",
    "their investment once a drawdown exceeds 30%. In a retail setting the situation is very different.\n",
    "Individuals are likely to be able to suffer deeper drawdowns in the hope of gaining higher returns.\n",
    "\n",
    "    - Maximum Drawdown: The largest percentage drop from a previous peak to the current or previous trough in account equity.\n",
    "    - Drawdown duration\n",
    "    - Drawdown Curve: a time series plot of the strategy drawdown over the trading duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Risk and Money Management\n",
    "\n",
    "Managing risk usually comes in two flavours, firstly identifying and mitigating internal and external factors that\n",
    "can affect the performance or operation of an algorithmic trading strategy and secondly, how to\n",
    "optimally manage the strategy portfolio in order to maximise growth rate and minimise account\n",
    "drawdowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources of Risk\n",
    "\n",
    "- Strategy risk (model risk)\n",
    "- Portfolio risk: For instance, an equities portfolio may be extremely heavy on technology stocks and thus is extremely exposed to any issues that affect the tech sector as a whole. Running a portfolio of strategies brings up the issue of strategy correlation. In general, strategies should be designed to avoid correlation with each other by virtue of differing asset classes or time horizons.\n",
    "- Counterparty Risk\n",
    "- Operational Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Money Management\n",
    "\n",
    "We are in a situation where we can strike a balance between maximising long-term growth rate via leverage and minimising our \"risk\" by trying to limit the duration and extent of the drawdown. The major tool that will help us achieve this is called the Kelly Criterion. Let’s imagine that we have a set of N algorithmic\n",
    "trading strategies and we wish to determine both how to apply optimal leverage per strategy in\n",
    "order to maximise growth rate (but minimise drawdowns) and how to allocate capital between\n",
    "each strategy. If we denote the allocation between each strategy i as a vector f of length N, s.t.\n",
    "f = (f1; :::; fN), then the Kelly Criterion for optimal allocation to each strategy fi is given by:\n",
    "\n",
    "    fi = µi/σi**2\n",
    "    \n",
    "Where µi are the mean excess returns and σi are the standard deviation of excess returns for\n",
    "a strategy i. This formula essentially describes the optimal leverage that should be applied to\n",
    "each strategy. While the Kelly Criterion fi gives us the optimal leverage and strategy allocation, we still\n",
    "need to actually calculate our expected long-term compounded growth rate of the portfolio,\n",
    "which we denote by g. The formula for this is given by:\n",
    "\n",
    "    g = r + S**2/2\n",
    "    \n",
    "Where r is the risk-free interest rate, which is the rate at which you can borrow from the\n",
    "broker, and S is the annualised Sharpe Ratio of the strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Management\n",
    "\n",
    "Estimating the risk of loss to an algorithmic trading strategy, or portfolio of strategies, is of\n",
    "extreme importance for long-term capital growth. Many techniques for risk management have\n",
    "been developed for use in institutional settings. One technique in particular is known as Value\n",
    "at Risk or VaR: VaR provides an estimate, under a given degree of confidence, of the size of a loss from a portfolio over a given time period.\n",
    "- Standard Market Conditions - VaR is not supposed to consider extreme events or \"tail\n",
    "risk\", rather it is supposed to provide the expectation of a loss under normal \"day-to-day\"\n",
    "operation.\n",
    "- Volatilities and Correlations - VaR requires the volatilities of the assets under consideration, as well as their respective correlations. These two quantities are tricky to estimate\n",
    "and are subject to continual change.\n",
    "- Normality of Returns - VaR, in its standard form, assumes the returns of the asset or\n",
    "portfolio are normally distributed. This leads to more straightforward analytical calculation,\n",
    "but it is quite unrealistic for most assets.\n",
    "\n",
    "There are three techniques that will be of interest to us. The first\n",
    "is the variance-covariance method (using normality assumptions), the second is a Monte Carlo\n",
    "method (based on an underlying, potentially non-normal, distribution) and the third is known\n",
    "as historical bootstrapping, which makes use of historical returns information for assets under\n",
    "consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas_datareader as web\n",
    "from scipy.stats import norm\n",
    "def var_cov_var(P, c, mu, sigma):\n",
    "    \"\"\"\n",
    "    Variance-Covariance calculation of daily Value-at-Risk\n",
    "    using confidence level c, with mean of returns mu\n",
    "    and standard deviation of returns sigma, on a portfolio\n",
    "    of value P.\n",
    "    \"\"\"\n",
    "    alpha = norm.ppf(1-c, mu, sigma)\n",
    "    return P - P*(alpha + 1)\n",
    "if __name__ == \"__main__\":\n",
    "    start = datetime.datetime(2010, 1, 1)\n",
    "    end = datetime.datetime(2014, 1, 1)\n",
    "    citi = web.DataReader(\"C\", 'yahoo', start, end)\n",
    "    citi[\"rets\"] = citi[\"Adj Close\"].pct_change()\n",
    "    P = 1e6 # 1,000,000 USD\n",
    "    c = 0.99 # 99% confidence interval\n",
    "    mu = np.mean(citi[\"rets\"])\n",
    "    sigma = np.std(citi[\"rets\"])\n",
    "    var = var_cov_var(P, c, mu, sigma)\n",
    "    print(\"Value-at-Risk: $%0.2f\" % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Event-Driven Trading Engine Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event-driven systems run the entire set of calculations within an \"infinite\" loop known as the event-loop. At each tick of the game-loop a function is called to receive the latest event, which will have been generated by some corresponding prior action within the game. Depending upon the nature of the event, which could include a key-press or a mouse click, some subsequent action is taken, which will either terminate the loop or generate some additional events. In particular it allows the illusion of real-time response handling because the code is continually being looped and events checked for.\n",
    "\n",
    "    while True: # Run the loop forever\n",
    "        new_event = get_new_event() # Get the latest event\n",
    "        # Based on the event type, perform an action\n",
    "        if new_event.type == \"LEFT_MOUSE_CLICK\":\n",
    "            open_menu()\n",
    "        elif new_event.type == \"ESCAPE_KEY_PRESS\":\n",
    "            quit_game()\n",
    "        elif new_event.type == \"UP_KEY_PRESS\":\n",
    "            move_player_north()\n",
    "        # ... and many more events\n",
    "        redraw_screen() # Update the screen to provide animation\n",
    "        tick(50) # Wait 50 milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event-driven systems provide many advantages over a vectorised approach:\n",
    "\n",
    "1. Code Reuse - An event-driven backtester, by design, can be used for both historical backtesting and live trading with minimal switch-out of components. This is not true of vectorised backtesters where all data must be available at once to carry out statistical analysis.\n",
    "2. Lookahead Bias - With an event-driven backtester there is no lookahead bias as market data receipt is treated as an \"event\" that must be acted upon. Thus it is possible to \"drip feed\" an event-driven backtester with market data, replicating how an order management and portfolio system would behave.\n",
    "3. Realism - Event-driven backtesters allow significant customisation over how orders are executed and transaction costs are incurred. It is straightforward to handle basic market and limit orders, as well as market-on-open (MOO) and market-on-close (MOC), since a custom exchange handler can be constructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component Objects: \n",
    "1. Event - The Event is the fundamental class unit of the event-driven system. It contains\n",
    "a type (such as \"MARKET\", \"SIGNAL\", \"ORDER\" or \"FILL\") that determines how it\n",
    "will be handled within the event-loop.\n",
    "2. Event Queue - The Event Queue is an in-memory Python Queue object that stores all\n",
    "of the Event sub-class objects that are generated by the rest of the software.\n",
    "3. DataHandler - The DataHandler is an abstract base class (ABC) that presents an interface for handling both historical or live market data. This provides significant flexibility\n",
    "as the Strategy and Portfolio modules can thus be reused between both approaches. The\n",
    "DataHandler generates a new MarketEvent upon every heartbeat of the system.\n",
    "4. Strategy - The Strategy is also an ABC that presents an interface for taking market data\n",
    "and generating corresponding SignalEvents, which are ultimately utilised by the Portfolio\n",
    "object. A SignalEvent contains a ticker symbol, a direction (LONG or SHORT) and a\n",
    "timestamp.\n",
    "5. Portfolio - This is a class hierarchy which handles the order management associated\n",
    "with current and subsequent positions for a strategy. It also carries out risk management\n",
    "across the portfolio, including sector exposure and position sizing. In a more sophisticated\n",
    "implementation this could be delegated to a RiskManagement class. The Portfolio takes\n",
    "SignalEvents from the Queue and generates OrderEvents that get added to the Queue.\n",
    "6. ExecutionHandler - The ExecutionHandler simulates a connection to a brokerage. The\n",
    "job of the handler is to take OrderEvents from the Queue and execute them, either via a\n",
    "simulated approach or an actual connection to a liver brokerage. Once orders are executed\n",
    "the handler creates FillEvents, which describe what was actually transacted, including fees,\n",
    "commission and slippage (if modelled).\n",
    "7. Backtest - All of these components are wrapped in an event-loop that correctly handles\n",
    "all Event types, routing them to the appropriate component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event\n",
    "In this infrastructure there are four types of events which allow communication between the above components via an event queue. They are a MarketEvent, SignalEvent, OrderEvent and FillEvent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event\n",
    "class Event(object):\n",
    "    \"\"\"\n",
    "    Event is base class providing an interface for all subsequent\n",
    "    (inherited) events, that will trigger further events in the\n",
    "    trading infrastructure.It is a base class and does not provide any functionality or specific interface.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# market event\n",
    "class MarketEvent(Event):\n",
    "    \"\"\"\n",
    "    Handles the event of receiving a new market update with\n",
    "    corresponding bars. It occurs when the DataHandler object receives a new update of market data\n",
    "    for any symbols which are currently being tracked.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises the MarketEvent.\"\"\"\n",
    "        self.type = 'MARKET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal event\n",
    "'''The Strategy object utilises market data to create new SignalEvents. The SignalEvent contains\n",
    "a strategy ID, a ticker symbol, a timestamp for when it was generated, a direction (long or short)\n",
    "and a \"strength\" indicator (this is useful for mean reversion strategies). The SignalEvents are\n",
    "utilised by the Portfolio object as advice for how to trade.\n",
    "'''\n",
    "class SignalEvent(Event):\n",
    "    \"\"\"\n",
    "    Handles the event of sending a Signal from a Strategy object.\n",
    "    This is received by a Portfolio object and acted upon.\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy_id, symbol, datetime, signal_type, strength):\n",
    "        \"\"\"\n",
    "        Initialises the SignalEvent.\n",
    "        Parameters:\n",
    "        strategy_id - The unique identifier for the strategy that generated the signal.\n",
    "        symbol - The ticker symbol, e.g. ’GOOG’.\n",
    "        datetime - The timestamp at which the signal was generated.\n",
    "        signal_type - ’LONG’ or ’SHORT’.\n",
    "        strength - An adjustment factor \"suggestion\" used to scale quantity at the portfolio level. \n",
    "                    Useful for pairs strategies.\n",
    "        \"\"\"\n",
    "        self.type = 'SIGNAL'\n",
    "        self.strategy_id = strategy_id\n",
    "        self.symbol = symbol\n",
    "        self.datetime = datetime\n",
    "        self.signal_type = signal_type\n",
    "        self.strength = strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order event\n",
    "'''When a Portfolio object receives SignalEvents it assesses them in the wider context of the portfolio, \n",
    "in terms of risk and position sizing. This ultimately leads to OrderEvents that will be sent\n",
    "to an ExecutionHandler.\n",
    "'''\n",
    "class OrderEvent(Event):\n",
    "    \"\"\"\n",
    "    Handles the event of sending an Order to an execution system.\n",
    "    The order contains a symbol (e.g. GOOG), a type (market or limit),\n",
    "    quantity and a direction.\n",
    "    \"\"\"\n",
    "    def __init__(self, symbol, order_type, quantity, direction):\n",
    "        \"\"\"\n",
    "        Initialises the order type, setting whether it is\n",
    "        a Market order (’MKT’) or Limit order (’LMT’), has\n",
    "        a quantity (integral) and its direction (’BUY’ or\n",
    "        ’SELL’).\n",
    "        Parameters:\n",
    "        symbol - The instrument to trade.\n",
    "        order_type - ’MKT’ or ’LMT’ for Market or Limit.\n",
    "        quantity - Non-negative integer for quantity.\n",
    "        direction - ’BUY’ or ’SELL’ for long or short.\n",
    "        \"\"\"\n",
    "        self.type = 'ORDER'\n",
    "        self.symbol = symbol\n",
    "        self.order_type = order_type\n",
    "        self.quantity = quantity\n",
    "        self.direction = direction\n",
    "    \n",
    "    def print_order(self):\n",
    "        \"\"\"\n",
    "        Outputs the values within the Order.\n",
    "        \"\"\"\n",
    "        print(\n",
    "            \"Order: Symbol=%s, Type=%s, Quantity=%s, Direction=%s\" %\n",
    "            (self.symbol, self.order_type, self.quantity, self.direction)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillEvent(Event):\n",
    "    \"\"\"\n",
    "    Encapsulates the notion of a Filled Order, as returned\n",
    "    from a brokerage. Stores the quantity of an instrument\n",
    "    actually filled and at what price. In addition, stores\n",
    "    the commission of the trade from the brokerage.\n",
    "    \"\"\"\n",
    "    def __init__(self, timeindex, symbol, exchange, quantity,\n",
    "                direction, fill_cost, commission=None):\n",
    "        \"\"\"\n",
    "        Initialises the FillEvent object. Sets the symbol, exchange,\n",
    "        quantity, direction, cost of fill and an optional\n",
    "        commission.\n",
    "        If commission is not provided, the Fill object will\n",
    "        calculate it based on the trade size and Interactive\n",
    "        Brokers fees.\n",
    "        Parameters:\n",
    "        timeindex - The bar-resolution when the order was filled.\n",
    "        symbol - The instrument which was filled.\n",
    "        exchange - The exchange where the order was filled.\n",
    "        quantity - The filled quantity.\n",
    "        direction - The direction of fill (’BUY’ or ’SELL’)\n",
    "        fill_cost - The holdings value in dollars.\n",
    "        commission - An optional commission sent from IB.\n",
    "        \"\"\"\n",
    "        self.type = 'FILL'\n",
    "        self.timeindex = timeindex\n",
    "        self.symbol = symbol\n",
    "        self.exchange = exchange\n",
    "        self.quantity = quantity\n",
    "        self.direction = direction\n",
    "        self.fill_cost = fill_cost\n",
    "        \n",
    "        # Calculate commission\n",
    "        if commission is None:\n",
    "            self.commission = self.calculate_ib_commission()\n",
    "        else:\n",
    "            self.commission = commission\n",
    "    \n",
    "    def calculate_ib_commission(self):\n",
    "        \"\"\"\n",
    "        Calculates the fees of trading based on an Interactive\n",
    "        Brokers fee structure for API, in USD.\n",
    "        This does not include exchange or ECN fees.\n",
    "        Based on \"US API Directed Orders\":\n",
    "        https://www.interactivebrokers.com/en/index.php?\n",
    "        f=commission&p=stocks2\n",
    "        \"\"\"\n",
    "        full_cost = 1.3\n",
    "        if self.quantity <= 500:\n",
    "            full_cost = max(1.3, 0.013 * self.quantity)\n",
    "        else: # Greater than 500\n",
    "            full_cost = max(1.3, 0.008 * self.quantity)\n",
    "        return full_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handler\n",
    "One of the goals of an event-driven trading system is to minimise duplication of code between\n",
    "the backtesting element and the live execution element. Specific example subclasses could include HistoricCSVDataHandler, QuandlDataHandler, SecuritiesMasterDataHandler, InteractiveBrokersMarketFeedDataHandler etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import datetime\n",
    "import os, os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from event import MarketEvent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataHandler is an abstract base class (ABC), which means that it is impossible to\n",
    "instantiate an instance directly. Only subclasses may be instantiated. The rationale for this is\n",
    "that the ABC provides an interface that all subsequent DataHandler subclasses must adhere to\n",
    "thereby ensuring compatibility with other classes that communicate with them. We make use of the \"__ metaclass __\"\n",
    "property to let Python know that this is an ABC. In\n",
    "addition we use the @abstractmethod decorator to let Python know that the method will be\n",
    "overridden in subclasses (this is identical to a pure virtual method in C++).\n",
    "\n",
    "The first two methods, get_latest_bar and\n",
    "get_latest_bars, are used to retrieve a recent subset of the historical trading bars from a stored\n",
    "list of such bars. These methods come in handy within the Strategy and Portfolio classes, due\n",
    "to the need to constantly be aware of current market prices and volumes.\n",
    "\n",
    "The following two methods, get_latest_bar_value and get_latest_bar_values, are convenience methods used to retrieve individual values from a particular bar, or list of bars. For\n",
    "instance it is often the case that a strategy is only interested in closing prices. In this instance\n",
    "we can use these methods to return a list of floating point values representing the closing prices of\n",
    "previous bars, rather than having to obtain it from the list of bar objects. This generally increases\n",
    "efficiency of strategies that utilise a \"lookback window\", such as those involving regressions.\n",
    "\n",
    "The final method, update_bars, provides a \"drip feed\" mechanism for placing bar information on a new data structure that strictly prohibits lookahead bias. This is one of the key\n",
    "differences between an event-driven backtesting system and one based on vectorisation.\n",
    "\n",
    "Notice\n",
    "that exceptions will be raised if an attempted instantiation of the class occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler(object):\n",
    "    \"\"\"\n",
    "    DataHandler is an abstract base class providing an interface for\n",
    "    all subsequent (inherited) data handlers (both live and historic).\n",
    "    The goal of a (derived) DataHandler object is to output a generated\n",
    "    set of bars (OHLCVI) for each symbol requested.\n",
    "    This will replicate how a live strategy would function as current\n",
    "    market data would be sent \"down the pipe\". Thus a historic and live\n",
    "    system will be treated identically by the rest of the backtesting suite.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "   \n",
    "    @abstractmethod\n",
    "    def get_latest_bar(self, symbol):\n",
    "        \"\"\"\n",
    "        Returns the last bar updated.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement get_latest_bar()\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_latest_bars(self, symbol, N=1):\n",
    "        \"\"\"\n",
    "        Returns the last N bars updated.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement get_latest_bars()\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_latest_bar_datetime(self, symbol):\n",
    "        \"\"\"\n",
    "        Returns a Python datetime object for the last bar.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement get_latest_bar_datetime()\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_latest_bar_value(self, symbol, val_type):\n",
    "        \"\"\"\n",
    "        Returns one of the Open, High, Low, Close, Volume or OI\n",
    "        from the last bar.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement get_latest_bar_value()\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_latest_bars_values(self, symbol, val_type, N=1):\n",
    "        \"\"\"\n",
    "        Returns the last N bar values from the\n",
    "        latest_symbol list, or N-k if less available.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement get_latest_bars_values()\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def update_bars(self):\n",
    "        \"\"\"\n",
    "        Pushes the latest bars to the bars_queue for each symbol\n",
    "        in a tuple OHLCVI format: (datetime, open, high, low,\n",
    "        close, volume, open interest).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement update_bars()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define the HistoricCSVDataHandler subclass, which is designed to\n",
    "process multiple CSV files, one for each traded symbol, and convert these into a dictionary of\n",
    "pandas DataFrames that can be accessed by the previously mentioned bar methods.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoricCSVDataHandler(DataHandler):\n",
    "    \"\"\"\n",
    "    HistoricCSVDataHandler is designed to read CSV files for\n",
    "    each requested symbol from disk and provide an interface\n",
    "    to obtain the \"latest\" bar in a manner identical to a live\n",
    "    trading interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, events, csv_dir, symbol_list):\n",
    "        \"\"\"\n",
    "        Initialises the historic data handler by requesting\n",
    "        the location of the CSV files and a list of symbols.\n",
    "        It will be assumed that all files are of the form\n",
    "        ’symbol.csv’, where symbol is a string in the list.\n",
    "        Parameters:\n",
    "        events - The Event Queue.\n",
    "        csv_dir - Absolute directory path to the CSV files.\n",
    "        symbol_list - A list of symbol strings.\n",
    "        \"\"\"\n",
    "        self.events = events\n",
    "        self.csv_dir = csv_dir\n",
    "        self.symbol_list = symbol_list\n",
    "        self.symbol_data = {}\n",
    "        self.latest_symbol_data = {}\n",
    "        self.continue_backtest = True\n",
    "        self._open_convert_csv_files()\n",
    "    \n",
    "    def _open_convert_csv_files(self):\n",
    "        \"\"\"\n",
    "        Opens the CSV files from the data directory, converting\n",
    "        them into pandas DataFrames within a symbol dictionary.\n",
    "        For this handler it will be assumed that the data is\n",
    "        taken from Yahoo. Thus its format will be respected.\n",
    "        \"\"\"\n",
    "        comb_index = None\n",
    "        for s in self.symbol_list:\n",
    "            # Load the CSV file with no header information, indexed on date\n",
    "            self.symbol_data[s] = pd.io.parsers.read_csv(\n",
    "                os.path.join(self.csv_dir, ’%s.csv’ % s),\n",
    "                header=0, index_col=0, parse_dates=True,\n",
    "                names=[\n",
    "                    ’datetime’, ’open’, ’high’,\n",
    "                    ’low’, ’close’, ’volume’, ’adj_close’\n",
    "                ]\n",
    "            ).sort()\n",
    "            # Combine the index to pad forward values\n",
    "            if comb_index is None:\n",
    "                comb_index = self.symbol_data[s].index\n",
    "            else:\n",
    "                comb_index.union(self.symbol_data[s].index)\n",
    "            # Set the latest symbol_data to None\n",
    "            self.latest_symbol_data[s] = []\n",
    "        # Reindex the dataframes\n",
    "        for s in self.symbol_list:\n",
    "            self.symbol_data[s] = self.symbol_data[s].\\\n",
    "                reindex(index=comb_index, method='pad').iterrows()\n",
    "    \n",
    "    def _get_new_bar(self, symbol):\n",
    "        \"\"\"\n",
    "        Returns the latest bar from the data feed.\n",
    "        \"\"\"\n",
    "        for b in self.symbol_data[symbol]:\n",
    "            yield b\n",
    "    \n",
    "    def get_latest_bar(self, symbol):\n",
    "        \"\"\"\n",
    "        Returns the last bar from the latest_symbol list.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bars_list = self.latest_symbol_data[symbol]\n",
    "        except KeyError:\n",
    "            print(\"That symbol is not available in the historical data set.\")\n",
    "            raise\n",
    "        else:\n",
    "            return bars_list[-1]\n",
    "    \n",
    "    def get_latest_bars(self, symbol, N=1):\n",
    "        \"\"\"\n",
    "        Returns the last N bars from the latest_symbol list,\n",
    "        or N-k if less available.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bars_list = self.latest_symbol_data[symbol]\n",
    "        except KeyError:\n",
    "            print(\"That symbol is not available in the historical data set.\")\n",
    "            raise\n",
    "        else:\n",
    "            return bars_list[-N:]\n",
    "    \n",
    "    def get_latest_bar_datetime(self, symbol):\n",
    "        \"\"\"\n",
    "        Returns a Python datetime object for the last bar.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bars_list = self.latest_symbol_data[symbol]\n",
    "        except KeyError:\n",
    "            print(\"That symbol is not available in the historical data set.\")\n",
    "            raise\n",
    "        else:\n",
    "            return bars_list[-1][0]\n",
    "    \n",
    "    def get_latest_bar_value(self, symbol, val_type):\n",
    "        \"\"\"\n",
    "        Returns one of the Open, High, Low, Close, Volume or OI\n",
    "        values from the pandas Bar series object.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bars_list = self.latest_symbol_data[symbol]\n",
    "        except KeyError:\n",
    "            print(\"That symbol is not available in the historical data set.\")\n",
    "            raise\n",
    "        else:\n",
    "            return getattr(bars_list[-1][1], val_type)\n",
    "    \n",
    "    def get_latest_bars_values(self, symbol, val_type, N=1):\n",
    "        \"\"\"\n",
    "        Returns the last N bar values from the\n",
    "        latest_symbol list, or N-k if less available.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bars_list = self.get_latest_bars(symbol, N)\n",
    "        except KeyError:\n",
    "            print(\"That symbol is not available in the historical data set.\")\n",
    "            raise\n",
    "        else:\n",
    "            return np.array([getattr(b[1], val_type) for b in bars_list])        \n",
    "    \n",
    "    def update_bars(self):\n",
    "        \"\"\"\n",
    "        Pushes the latest bar to the latest_symbol_data structure\n",
    "        for all symbols in the symbol list.\n",
    "        \"\"\"\n",
    "        for s in self.symbol_list:\n",
    "            try:\n",
    "                bar = next(self._get_new_bar(s))\n",
    "            except StopIteration:\n",
    "                self.continue_backtest = False\n",
    "            else:\n",
    "                if bar is not None:\n",
    "                    self.latest_symbol_data[s].append(bar)\n",
    "        self.events.put(MarketEvent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "A Strategy object encapsulates all calculation on market data that generate advisory signals\n",
    "to a Portfolio object. At this stage in the event-driven backtester development there is no concept of an indicator\n",
    "or filter, such as those found in technical trading. These are also good candidates for creating a class hierarchy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import datetime\n",
    "try:\n",
    "    import Queue as queue\n",
    "except ImportError:\n",
    "    import queue\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from event import SignalEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy(object):\n",
    "    \"\"\"\n",
    "    Strategy is an abstract base class providing an interface for\n",
    "    all subsequent (inherited) strategy handling objects.\n",
    "    The goal of a (derived) Strategy object is to generate Signal\n",
    "    objects for particular symbols based on the inputs of Bars\n",
    "    (OHLCV) generated by a DataHandler object.\n",
    "    This is designed to work both with historic and live data as\n",
    "    the Strategy object is agnostic to where the data came from,\n",
    "    since it obtains the bar tuples from a queue object.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def calculate_signals(self):\n",
    "        \"\"\"\n",
    "        Provides the mechanisms to calculate the list of signals.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement calculate_signals()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portfolio\n",
    "This section describes a Portfolio object that keeps track of the positions within a portfolio\n",
    "and generates orders of a fixed quantity of stock based on signals. More sophisticated portfolio\n",
    "objects could include risk management and position sizing tools (such as the Kelly Criterion).\n",
    "\n",
    "The portfolio order management system is possibly the most complex component of an eventdriven backtester. Its role is to keep track of all current market positions as well as the market\n",
    "value of the positions (known as the \"holdings\"). This is simply an estimate of the liquidation\n",
    "value of the position and is derived in part from the data handling facility of the backtester.\n",
    "\n",
    "We create a new file portfolio.py and import the necessary libraries. These are the same\n",
    "as most of the other class implementations, with the exception that Portfolio is NOT going to\n",
    "be an abstract base class. Instead it will be normal base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_sharpe_ratio(returns, periods=252):\n",
    "    \"\"\"\n",
    "    Create the Sharpe ratio for the strategy, based on a\n",
    "    benchmark of zero (i.e. no risk-free rate information).\n",
    "    Parameters:\n",
    "    returns - A pandas Series representing period percentage returns.\n",
    "    periods - Daily (252), Hourly (252*6.5), Minutely(252*6.5*60) etc.\n",
    "    \"\"\"\n",
    "    return np.sqrt(periods) * (np.mean(returns)) / np.std(returns)\n",
    "\n",
    "def create_drawdowns(pnl):\n",
    "    \"\"\"\n",
    "    Calculate the largest peak-to-trough drawdown of the PnL curve\n",
    "    as well as the duration of the drawdown. Requires that the\n",
    "    pnl_returns is a pandas Series.\n",
    "    Parameters:\n",
    "    pnl - A pandas Series representing period percentage returns.\n",
    "    Returns:\n",
    "    drawdown, duration - Highest peak-to-trough drawdown and duration.\n",
    "    \"\"\"\n",
    "    # Calculate the cumulative returns curve\n",
    "    # and set up the High Water Mark\n",
    "    hwm = [0]\n",
    "    # Create the drawdown and duration series\n",
    "    idx = pnl.index\n",
    "    drawdown = pd.Series(index = idx)\n",
    "    duration = pd.Series(index = idx)\n",
    "    # Loop over the index range\n",
    "    for t in range(1, len(idx)):\n",
    "        hwm.append(max(hwm[t-1], pnl[t]))\n",
    "        drawdown[t]= (hwm[t]-pnl[t])\n",
    "        duration[t]= (0 if drawdown[t] == 0 else duration[t-1]+1)\n",
    "    return drawdown, drawdown.max(), duration.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio.py\n",
    "import datetime\n",
    "from math import floor\n",
    "try:\n",
    "    import Queue as queue\n",
    "except ImportError:\n",
    "    import queue\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from event import FillEvent, OrderEvent\n",
    "from performance import create_sharpe_ratio, create_drawdowns\n",
    "\n",
    "class Portfolio(object):\n",
    "    \"\"\"\n",
    "    The Portfolio class handles the positions and market\n",
    "    value of all instruments at a resolution of a \"bar\",\n",
    "    i.e. secondly, minutely, 5-min, 30-min, 60 min or EOD.\n",
    "    The positions DataFrame stores a time-index of the\n",
    "    quantity of positions held.\n",
    "    The holdings DataFrame stores the cash and total market\n",
    "    holdings value of each symbol for a particular\n",
    "    time-index, as well as the percentage change in\n",
    "    portfolio total across bars.\n",
    "    \"\"\"\n",
    "    def __init__(self, bars, events, start_date, initial_capital=100000.0):\n",
    "        \"\"\"\n",
    "        Initialises the portfolio with bars and an event queue.\n",
    "        Also includes a starting datetime index and initial capital\n",
    "        (USD unless otherwise stated).\n",
    "        Parameters:\n",
    "        bars - The DataHandler object with current market data.\n",
    "        events - The Event Queue object.\n",
    "        start_date - The start date (bar) of the portfolio.\n",
    "        initial_capital - The starting capital in USD.\n",
    "        \"\"\"\n",
    "        self.bars = bars\n",
    "        self.events = events\n",
    "        self.symbol_list = self.bars.symbol_list\n",
    "        self.start_date = start_date\n",
    "        self.initial_capital = initial_capital\n",
    "        self.all_positions = self.construct_all_positions()\n",
    "        self.current_positions = dict( (k,v) for k, v in \\\n",
    "            [(s, 0) for s in self.symbol_list] )\n",
    "        self.all_holdings = self.construct_all_holdings()\n",
    "        self.current_holdings = self.construct_current_holdings()\n",
    "    \n",
    "    def construct_all_positions(self):\n",
    "        \"\"\"\n",
    "        Constructs the positions list using the start_date\n",
    "        to determine when the time index will begin.\n",
    "        \"\"\"\n",
    "        d = dict( (k,v) for k, v in [(s, 0) for s in self.symbol_list] )\n",
    "        d['datetime'] = self.start_date\n",
    "        return [d]\n",
    "    \n",
    "    def construct_all_holdings(self):\n",
    "        \"\"\"\n",
    "        Constructs the holdings list using the start_date\n",
    "        to determine when the time index will begin.\n",
    "        \"\"\"\n",
    "        d = dict( (k,v) for k, v in [(s, 0.0) for s in self.symbol_list] )\n",
    "        d['datetime'] = self.start_date\n",
    "        d['cash'] = self.initial_capital\n",
    "        d['commission'] = 0.0\n",
    "        d['total'] = self.initial_capital\n",
    "        return [d]\n",
    "    \n",
    "    def construct_current_holdings(self):\n",
    "        \"\"\"\n",
    "        This constructs the dictionary which will hold the instantaneous\n",
    "        value of the portfolio across all symbols.\n",
    "        \"\"\"\n",
    "        d = dict( (k,v) for k, v in [(s, 0.0) for s in self.symbol_list] )\n",
    "        d['cash'] = self.initial_capital\n",
    "        d['commission'] = 0.0\n",
    "        d['total'] = self.initial_capital\n",
    "        return d\n",
    "    \n",
    "    def update_timeindex(self, event):\n",
    "        \"\"\"\n",
    "        Adds a new record to the positions matrix for the current\n",
    "        market data bar. This reflects the PREVIOUS bar, i.e. all\n",
    "        current market data at this stage is known (OHLCV).\n",
    "        Makes use of a MarketEvent from the events queue.\n",
    "        \"\"\"\n",
    "        latest_datetime = self.bars.get_latest_bar_datetime(\n",
    "            self.symbol_list[0]\n",
    "        )\n",
    "        # Update positions\n",
    "        # ================\n",
    "        dp = dict( (k,v) for k, v in [(s, 0) for s in self.symbol_list] )\n",
    "        dp[’datetime’] = latest_datetime\n",
    "        for s in self.symbol_list:\n",
    "            dp[s] = self.current_positions[s]\n",
    "        # Append the current positions\n",
    "        self.all_positions.append(dp)\n",
    "        # Update holdings\n",
    "        # ===============\n",
    "        dh = dict( (k,v) for k, v in [(s, 0) for s in self.symbol_list] )\n",
    "        dh['datetime'] = latest_datetime\n",
    "        dh['cash'] = self.current_holdings['cash']\n",
    "        dh['commission'] = self.current_holdings['commission']\n",
    "        dh['total'] = self.current_holdings['cash']\n",
    "        for s in self.symbol_list:\n",
    "            # Approximation to the real value\n",
    "            market_value = self.current_positions[s] * \\\n",
    "                self.bars.get_latest_bar_value(s, \"adj_close\")\n",
    "            dh[s] = market_value\n",
    "            dh['total'] += market_value\n",
    "        # Append the current holdings\n",
    "        self.all_holdings.append(dh)\n",
    "        \n",
    "    def update_positions_from_fill(self, fill):\n",
    "        \"\"\"\n",
    "        Takes a Fill object and updates the position matrix to\n",
    "        reflect the new position.\n",
    "        Parameters:\n",
    "        fill - The Fill object to update the positions with.\n",
    "        \"\"\"\n",
    "        # Check whether the fill is a buy or sell\n",
    "        fill_dir = 0\n",
    "        if fill.direction == 'BUY':\n",
    "            fill_dir = 1\n",
    "        if fill.direction == 'SELL':\n",
    "            fill_dir = -1\n",
    "        # Update positions list with new quantities\n",
    "        self.current_positions[fill.symbol] += fill_dir*fill.quantity\n",
    "    \n",
    "    def update_holdings_from_fill(self, fill):\n",
    "        \"\"\"\n",
    "        Takes a Fill object and updates the holdings matrix to\n",
    "        reflect the holdings value.\n",
    "        Parameters:\n",
    "        fill - The Fill object to update the holdings with.\n",
    "        \"\"\"\n",
    "        # Check whether the fill is a buy or sell\n",
    "        fill_dir = 0\n",
    "        if fill.direction == 'BUY':\n",
    "            fill_dir = 1\n",
    "        if fill.direction == 'SELL':\n",
    "            fill_dir = -1\n",
    "        # Update holdings list with new quantities\n",
    "        fill_cost = self.bars.get_latest_bar_value(fill.symbol, \"adj_close\")\n",
    "        cost = fill_dir * fill_cost * fill.quantity\n",
    "        self.current_holdings[fill.symbol] += cost\n",
    "        self.current_holdings[’commission’] += fill.commission\n",
    "        self.current_holdings[’cash’] -= (cost + fill.commission)\n",
    "        self.current_holdings[’total’] -= (cost + fill.commission)\n",
    "    \n",
    "    def update_fill(self, event):\n",
    "        \"\"\"\n",
    "        Updates the portfolio current positions and holdings\n",
    "        from a FillEvent.\n",
    "        \"\"\"\n",
    "        if event.type == 'FILL':\n",
    "            self.update_positions_from_fill(event)\n",
    "            self.update_holdings_from_fill(event)\n",
    "    \n",
    "    def generate_naive_order(self, signal):\n",
    "        \"\"\"\n",
    "        Simply files an Order object as a constant quantity\n",
    "        sizing of the signal object, without risk management or\n",
    "        position sizing considerations.\n",
    "        Parameters:\n",
    "        signal - The tuple containing Signal information.\n",
    "        \"\"\"\n",
    "        order = None\n",
    "        symbol = signal.symbol\n",
    "        direction = signal.signal_type\n",
    "        strength = signal.strength\n",
    "        mkt_quantity = 100\n",
    "        cur_quantity = self.current_positions[symbol]\n",
    "        order_type = 'MKT'\n",
    "        if direction == 'LONG' and cur_quantity == 0:\n",
    "            order = OrderEvent(symbol, order_type, mkt_quantity, 'BUY')\n",
    "        if direction == 'SHORT' and cur_quantity == 0:\n",
    "            order = OrderEvent(symbol, order_type, mkt_quantity, 'SELL')\n",
    "        if direction == 'EXIT' and cur_quantity > 0:\n",
    "            order = OrderEvent(symbol, order_type, abs(cur_quantity), 'SELL')\n",
    "        if direction == 'EXIT' and cur_quantity < 0:\n",
    "            order = OrderEvent(symbol, order_type, abs(cur_quantity), 'BUY')\n",
    "        return order\n",
    "    \n",
    "    def update_signal(self, event):\n",
    "        \"\"\"\n",
    "        Acts on a SignalEvent to generate new orders\n",
    "        based on the portfolio logic.\n",
    "        \"\"\"\n",
    "        if event.type == 'SIGNAL':\n",
    "            order_event = self.generate_naive_order(event)\n",
    "            self.events.put(order_event)\n",
    "    \n",
    "    def create_equity_curve_dataframe(self):\n",
    "        \"\"\"\n",
    "        Creates a pandas DataFrame from the all_holdings\n",
    "        list of dictionaries.\n",
    "        \"\"\"\n",
    "        curve = pd.DataFrame(self.all_holdings)\n",
    "        curve.set_index('datetime', inplace=True)\n",
    "        curve['returns'] = curve[’total’].pct_change()\n",
    "        curve['equity_curve'] = (1.0+curve['returns']).cumprod()\n",
    "        self.equity_curve = curve\n",
    "    \n",
    "    def output_summary_stats(self):\n",
    "        \"\"\"\n",
    "        Creates a list of summary statistics for the portfolio.\n",
    "        \"\"\"\n",
    "        total_return = self.equity_curve['equity_curve'][-1]\n",
    "        returns = self.equity_curve['returns']\n",
    "        pnl = self.equity_curve['equity_curve']\n",
    "        sharpe_ratio = create_sharpe_ratio(returns, periods=252*60*6.5)\n",
    "        drawdown, max_dd, dd_duration = create_drawdowns(pnl)\n",
    "        self.equity_curve['drawdown'] = drawdown\n",
    "        stats = [(\"Total Return\", \"%0.2f%%\" % \\\n",
    "                ((total_return - 1.0) * 100.0)),\n",
    "            (\"Sharpe Ratio\", \"%0.2f\" % sharpe_ratio),\n",
    "            (\"Max Drawdown\", \"%0.2f%%\" % (max_dd * 100.0)),\n",
    "            (\"Drawdown Duration\", \"%d\" % dd_duration)]\n",
    "        self.equity_curve.to_csv('equity.csv')\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Handler\n",
    "In order to backtest strategies we need to simulate how a trade will be transacted. The\n",
    "simplest possible implementation is to assume all orders are filled at the current market price for\n",
    "all quantities. This is clearly extremely unrealistic and a big part of improving backtest realism\n",
    "will come from designing more sophisticated models of slippage and market impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import datetime\n",
    "try:\n",
    "    import Queue as queue\n",
    "except ImportError:\n",
    "    import queue\n",
    "from event import FillEvent, OrderEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutionHandler(object):\n",
    "    \"\"\"\n",
    "    The ExecutionHandler abstract class handles the interaction\n",
    "    between a set of order objects generated by a Portfolio and\n",
    "    the ultimate set of Fill objects that actually occur in the\n",
    "    market.\n",
    "    The handlers can be used to subclass simulated brokerages\n",
    "    or live brokerages, with identical interfaces. This allows\n",
    "    strategies to be backtested in a very similar manner to the\n",
    "    live trading engine.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute_order(self, event):\n",
    "        \"\"\"\n",
    "        Takes an Order event and executes it, producing\n",
    "        a Fill event that gets placed onto the Events queue.\n",
    "        Parameters:\n",
    "        event - Contains an Event object with order information.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Should implement execute_order()\")\n",
    "        \n",
    "class SimulatedExecutionHandler(ExecutionHandler):\n",
    "    \"\"\"\n",
    "    The simulated execution handler simply converts all order\n",
    "    objects into their equivalent fill objects automatically\n",
    "    without latency, slippage or fill-ratio issues.\n",
    "    This allows a straightforward \"first go\" test of any strategy,\n",
    "    before implementation with a more sophisticated execution\n",
    "    handler.\n",
    "    \"\"\"\n",
    "    def __init__(self, events):\n",
    "        \"\"\"\n",
    "        Initialises the handler, setting the event queues\n",
    "        up internally.\n",
    "        Parameters:\n",
    "        events - The Queue of Event objects.\n",
    "        \"\"\"\n",
    "        self.events = events\n",
    "    \n",
    "    def execute_order(self, event):\n",
    "        \"\"\"\n",
    "        Simply converts Order objects into Fill objects naively,\n",
    "        i.e. without any latency, slippage or fill ratio problems.\n",
    "        Parameters:\n",
    "        event - Contains an Event object with order information.\n",
    "        \"\"\"\n",
    "        if event.type == 'ORDER':\n",
    "            fill_event = FillEvent(\n",
    "                datetime.datetime.utcnow(), event.symbol,\n",
    "                ’ARCA’, event.quantity, event.direction, None\n",
    "            )\n",
    "            self.events.put(fill_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Backtest object encapsulates\n",
    "the event-handling logic and essentially ties together all of the other classes that we have discussed\n",
    "above. The Backtest object is designed to carry out a nested while-loop event-driven system in\n",
    "order to handle the events placed on the Event Queue object. The outer while-loop is known as\n",
    "the \"heartbeat loop\" and decides the temporal resolution of the backtesting system. In a live\n",
    "environment this value will be a positive number, such as 600 seconds (every ten minutes). Thus\n",
    "the market data and positions will only be updated on this timeframe.\n",
    "\n",
    "For the backtester described here the \"heartbeat\" can be set to zero, irrespective of the\n",
    "strategy frequency, since the data is already available by virtue of the fact it is historical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pprint\n",
    "try:\n",
    "    import Queue as queue\n",
    "except ImportError:\n",
    "    import queue\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backtest(object):\n",
    "    \"\"\"\n",
    "    Enscapsulates the settings and components for carrying out\n",
    "    an event-driven backtest.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, csv_dir, symbol_list, initial_capital,\n",
    "        heartbeat, start_date, data_handler,\n",
    "        execution_handler, portfolio, strategy\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the backtest.\n",
    "        Parameters:\n",
    "        csv_dir - The hard root to the CSV data directory.\n",
    "        symbol_list - The list of symbol strings.\n",
    "        intial_capital - The starting capital for the portfolio.\n",
    "        heartbeat - Backtest \"heartbeat\" in seconds\n",
    "        start_date - The start datetime of the strategy.\n",
    "        data_handler - (Class) Handles the market data feed.\n",
    "        execution_handler - (Class) Handles the orders/fills for trades.\n",
    "        portfolio - (Class) Keeps track of portfolio current\n",
    "        and prior positions.\n",
    "        strategy - (Class) Generates signals based on market data.\n",
    "        \"\"\"\n",
    "        self.csv_dir = csv_dir\n",
    "        self.symbol_list = symbol_list\n",
    "        self.initial_capital = initial_capital\n",
    "        self.heartbeat = heartbeat\n",
    "        self.start_date = start_date\n",
    "        self.data_handler_cls = data_handler\n",
    "        self.execution_handler_cls = execution_handler\n",
    "        self.portfolio_cls = portfolio\n",
    "        self.strategy_cls = strategy\n",
    "        self.events = queue.Queue()\n",
    "        self.signals = 0\n",
    "        self.orders = 0\n",
    "        self.fills = 0\n",
    "        self.num_strats = 1\n",
    "        self._generate_trading_instances()\n",
    "    \n",
    "    def _generate_trading_instances(self):\n",
    "        \"\"\"\n",
    "        Generates the trading instance objects from\n",
    "        their class types.\n",
    "        \"\"\"\n",
    "        print(\n",
    "            \"Creating DataHandler, Strategy, Portfolio and ExecutionHandler\"\n",
    "        )\n",
    "        self.data_handler = self.data_handler_cls(self.events, self.csv_dir,\n",
    "            self.symbol_list)\n",
    "        self.strategy = self.strategy_cls(self.data_handler, self.events)\n",
    "        self.portfolio = self.portfolio_cls(self.data_handler, self.events,\n",
    "            self.start_date,\n",
    "            self.initial_capital)\n",
    "        self.execution_handler = self.execution_handler_cls(self.events)\n",
    "    \n",
    "    def _run_backtest(self):\n",
    "        \"\"\"\n",
    "        Executes the backtest.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        while True:\n",
    "            i += 1\n",
    "            print i\n",
    "            # Update the market bars\n",
    "            if self.data_handler.continue_backtest == True:\n",
    "                self.data_handler.update_bars()\n",
    "            else:\n",
    "                break\n",
    "            # Handle the events\n",
    "            while True:\n",
    "                try:\n",
    "                    event = self.events.get(False)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "                else:\n",
    "                    if event is not None:\n",
    "                        if event.type == ’MARKET’:\n",
    "                            self.strategy.calculate_signals(event)\n",
    "                            self.portfolio.update_timeindex(event)\n",
    "                        elif event.type == ’SIGNAL’:\n",
    "                            self.signals += 1\n",
    "                            self.portfolio.update_signal(event)\n",
    "                        elif event.type == ’ORDER’:\n",
    "                            self.orders += 1\n",
    "                            self.execution_handler.execute_order(event)\n",
    "                        elif event.type == ’FILL’:\n",
    "                            self.fills += 1\n",
    "                            self.portfolio.update_fill(event)\n",
    "            time.sleep(self.heartbeat)\n",
    "            \n",
    "    def _output_performance(self):\n",
    "        \"\"\"\n",
    "        Outputs the strategy performance from the backtest.\n",
    "        \"\"\"\n",
    "        self.portfolio.create_equity_curve_dataframe()\n",
    "        print(\"Creating summary stats...\")\n",
    "        stats = self.portfolio.output_summary_stats()\n",
    "        print(\"Creating equity curve...\")\n",
    "        print(self.portfolio.equity_curve.tail(10))\n",
    "        pprint.pprint(stats)\n",
    "        print(\"Signals: %s\" % self.signals)\n",
    "        print(\"Orders: %s\" % self.orders)\n",
    "        print(\"Fills: %s\" % self.fills)\n",
    "        \n",
    "    def simulate_trading(self):\n",
    "        \"\"\"\n",
    "        Simulates the backtest and outputs portfolio performance.\n",
    "        \"\"\"\n",
    "        self._run_backtest()\n",
    "        self._output_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Trading Strategy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are going to consider the full implementation of trading strategies using the\n",
    "aforementioned event-driven backtesting system. In particular we will generate equity curves\n",
    "for all trading strategies using notional portfolio amounts, thus simulating the concepts of\n",
    "margin/leverage, which is a far more realistic approach compared to vectorised/returns based\n",
    "approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average Crossover Strategy\n",
    "In order to actually generate such a simulation based on the prior backtesting code we need to\n",
    "subclass the Strategy object as described in the previous chapter to create the MovingAverageCrossStrategy\n",
    "object, which will contain the logic of the simple moving averages and the generation of trading\n",
    "signals. In addition we need to create the __main__ function that will load the Backtest object and\n",
    "actually encapsulate the execution of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mac.py\n",
    "from __future__ import print_function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from strategy import Strategy\n",
    "from event import SignalEvent\n",
    "from backtest import Backtest\n",
    "from data import HistoricCSVDataHandler\n",
    "from execution import SimulatedExecutionHandler\n",
    "from portfolio import Portfolio\n",
    "class MovingAverageCrossStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    Carries out a basic Moving Average Crossover strategy with a\n",
    "    short/long simple weighted moving average. Default short/long\n",
    "    windows are 100/400 periods respectively.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, bars, events, short_window=100, long_window=400\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the Moving Average Cross Strategy.\n",
    "        Parameters:\n",
    "        bars - The DataHandler object that provides bar information\n",
    "        events - The Event Queue object.\n",
    "        short_window - The short moving average lookback.\n",
    "        long_window - The long moving average lookback.\n",
    "        \"\"\"\n",
    "        self.bars = bars\n",
    "        self.symbol_list = self.bars.symbol_list\n",
    "        self.events = events\n",
    "        self.short_window = short_window\n",
    "        self.long_window = long_window\n",
    "        # Set to True if a symbol is in the market\n",
    "        self.bought = self._calculate_initial_bought()\n",
    "    def _calculate_initial_bought(self):\n",
    "        \"\"\"\n",
    "        Adds keys to the bought dictionary for all symbols\n",
    "        and sets them to ’OUT’.\n",
    "        \"\"\"\n",
    "        bought = {}\n",
    "        for s in self.symbol_list:\n",
    "            bought[s] = ’OUT’\n",
    "        return bought\n",
    "    def calculate_signals(self, event):\n",
    "        \"\"\"\n",
    "        Generates a new set of signals based on the MAC\n",
    "        SMA with the short window crossing the long window\n",
    "        meaning a long entry and vice versa for a short entry.\n",
    "        Parameters\n",
    "        event - A MarketEvent object.\n",
    "        \"\"\"\n",
    "        if event.type == ’MARKET’:\n",
    "            for s in self.symbol_list:\n",
    "                bars = self.bars.get_latest_bars_values(\n",
    "                    s, \"adj_close\", N=self.long_window\n",
    "                )\n",
    "                bar_date = self.bars.get_latest_bar_datetime(s)\n",
    "                if bars is not None and bars != []:\n",
    "                    short_sma = np.mean(bars[-self.short_window:])\n",
    "                    long_sma = np.mean(bars[-self.long_window:])\n",
    "                    symbol = s\n",
    "                    dt = datetime.datetime.utcnow()\n",
    "                    sig_dir = \"\"\n",
    "                    if short_sma > long_sma and self.bought[s] == \"OUT\":\n",
    "                        print(\"LONG: %s\" % bar_date)\n",
    "                        sig_dir = ’LONG’\n",
    "                        signal = SignalEvent(1, symbol, dt, sig_dir, 1.0)\n",
    "                    self.events.put(signal)\n",
    "                    self.bought[s] = ’LONG’\n",
    "                elif short_sma < long_sma and self.bought[s] == \"LONG\":\n",
    "                    print(\"SHORT: %s\" % bar_date)\n",
    "                    sig_dir = ’EXIT’\n",
    "                    signal = SignalEvent(1, symbol, dt, sig_dir, 1.0)\n",
    "                    self.events.put(signal)\n",
    "                    self.bought[s] = ’OUT’\n",
    "if __name__ == \"__main__\":\n",
    "    csv_dir = ’/path/to/your/csv/file’ # CHANGE THIS!\n",
    "    symbol_list = [’AAPL’]\n",
    "    initial_capital = 100000.0\n",
    "    heartbeat = 0.0\n",
    "    start_date = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "    backtest = Backtest(\n",
    "        csv_dir, symbol_list, initial_capital, heartbeat,\n",
    "        start_date, HistoricCSVDataHandler, SimulatedExecutionHandler,\n",
    "        Portfolio, MovingAverageCrossStrategy\n",
    "    )\n",
    "    backtest.simulate_trading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python mac.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S&P500 Forecasting\n",
    "The rules for this strategy are as follows:\n",
    "1. Fit a forecasting model to a subset of S&P500 data. This could be Logistic Regression,\n",
    "a Discriminant Analyser (Linear or Quadratic), a Support Vector Machine or a Random\n",
    "Forest. The procedure to do this was outlined in the Forecasting chapter.\n",
    "2. Use two prior lags of adjusted closing returns data as a predictor for tomorrow’s returns. If\n",
    "the returns are predicted as positive then go long. If the returns are predicted as negative\n",
    "then exit. We’re not going to consider short selling for this particular strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.qda import QDA\n",
    "from strategy import Strategy\n",
    "from event import SignalEvent\n",
    "from backtest import Backtest\n",
    "from data import HistoricCSVDataHandler\n",
    "from execution import SimulatedExecutionHandler\n",
    "from portfolio import Portfolio\n",
    "from create_lagged_series import create_lagged_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPYDailyForecastStrategy(Strategy):\n",
    "\"\"\"S&P500 forecast strategy. It uses a Quadratic Discriminant\n",
    "Analyser to predict the returns for a subsequent time\n",
    "period and then generated long/exit signals based on the\n",
    "prediction.\n",
    "\"\"\"\n",
    "def __init__(self, bars, events):\n",
    "    self.bars = bars\n",
    "    self.symbol_list = self.bars.symbol_list\n",
    "    self.events = events\n",
    "    self.datetime_now = datetime.datetime.utcnow()\n",
    "    self.model_start_date = datetime.datetime(2001,1,10)\n",
    "    self.model_end_date = datetime.datetime(2005,12,31)\n",
    "    self.model_start_test_date = datetime.datetime(2005,1,1)\n",
    "    self.long_market = False\n",
    "    self.short_market = False\n",
    "    self.bar_index = 0\n",
    "    self.model = self.create_symbol_forecast_model()\n",
    "\n",
    "def create_symbol_forecast_model(self):\n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\n",
    "        self.symbol_list[0], self.model_start_date,\n",
    "        self.model_end_date, lags=5\n",
    "    )\n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    # Create training and test sets\n",
    "    start_test = self.model_start_test_date\n",
    "    X_train = X[X.index < start_test]\n",
    "    X_test = X[X.index >= start_test]\n",
    "    y_train = y[y.index < start_test]\n",
    "    y_test = y[y.index >= start_test]\n",
    "    model = QDA()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def calculate_signals(self, event):\n",
    "    \"\"\"\n",
    "    Calculate the SignalEvents based on market data.\n",
    "    \"\"\"\n",
    "    sym = self.symbol_list[0]\n",
    "    dt = self.datetime_now\n",
    "    if event.type == ’MARKET’:\n",
    "        self.bar_index += 1\n",
    "        if self.bar_index > 5:\n",
    "            lags = self.bars.get_latest_bars_values(\n",
    "                self.symbol_list[0], \"returns\", N=3\n",
    "            )\n",
    "            pred_series = pd.Series(\n",
    "                {\n",
    "                    ’Lag1’: lags[1]*100.0,\n",
    "                    ’Lag2’: lags[2]*100.0\n",
    "                }\n",
    "            )\n",
    "            pred = self.model.predict(pred_series)\n",
    "            if pred > 0 and not self.long_market:\n",
    "                self.long_market = True\n",
    "                signal = SignalEvent(1, sym, dt, ’LONG’, 1.0)\n",
    "                self.events.put(signal)\n",
    "            if pred < 0 and self.long_market:\n",
    "                self.long_market = False\n",
    "                signal = SignalEvent(1, sym, dt, ’EXIT’, 1.0)\n",
    "                self.events.put(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_dir = ’/path/to/your/csv/file’ # CHANGE THIS!\n",
    "    symbol_list = [’SPY’]\n",
    "    initial_capital = 100000.0\n",
    "    heartbeat = 0.0\n",
    "    start_date = datetime.datetime(2006,1,3)\n",
    "    backtest = Backtest(\n",
    "        csv_dir, symbol_list, initial_capital, heartbeat,\n",
    "        start_date, HistoricCSVDataHandler, SimulatedExecutionHandler,\n",
    "        Portfolio, SPYDailyForecastStrategy\n",
    "    )\n",
    "    backtest.simulate_trading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Reverting Equity Pairs Trade\n",
    "In order to seek higher Sharpe ratios for our trading, we need to consider higher-frequency\n",
    "intraday strategies. The first major issue is that obtaining data is significantly less straightforward because high\n",
    "quality intraday data is usually not free. As stated above I use DTN IQFeed for intraday minutely\n",
    "bars and thus you will need your own DTN account to obtain the data required for this strategy.\n",
    "The second issue is that backtesting simulations take substantially longer, especially with\n",
    "the event-driven model that we have constructed here. Once we begin considering a backtest\n",
    "of a diversified portfolio of minutely data spanning years, and then performing any parameter\n",
    "optimisation, we rapidly realise that simulations can take hours or even days to calculate on a\n",
    "modern desktop PC. This will need to be factored in to your research process.\n",
    "The third issue is that live execution will now need to be fully automated since we are edging\n",
    "into higher-frequency trading. This means that such execution environments and code must be\n",
    "highly reliable and bug-free, otherwise the potential for significant losses can occur.\n",
    "\n",
    "The rules for the strategy are straightforward:\n",
    "1. Identify a pair of equities that possess a residuals time series which has been statistically\n",
    "identified as mean-reverting. In this case, I have found two energy sector US equities with\n",
    "tickers AREX and WLL.\n",
    "2. Create the residuals time series of the pair by performing a rolling linear regression, for a\n",
    "particular lookback window, via the ordinary least squares (OLS) algorithm. This lookback\n",
    "period is a parameter to be optimised.\n",
    "3. Create a rolling z-score of the residuals time series of the same lookback period and use\n",
    "this to determine entry/exit thresholds for trading signals.\n",
    "4. If the upper threshold is exceeded when not in the market then enter the market (long or\n",
    "short depending on direction of threshold excess). If the lower threshold is exceeded when\n",
    "in the market, exit the market. Once again, the upper and lower thresholds are parameters\n",
    "to be optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from strategy import Strategy\n",
    "from event import SignalEvent\n",
    "from backtest import Backtest\n",
    "from hft_data import HistoricCSVDataHandlerHFT\n",
    "from hft_portfolio import PortfolioHFT\n",
    "from execution import SimulatedExecutionHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntradayOLSMRStrategy(Strategy):\n",
    "    \"\"\"\n",
    "    Uses ordinary least squares (OLS) to perform a rolling linear\n",
    "    regression to determine the hedge ratio between a pair of equities.\n",
    "    The z-score of the residuals time series is then calculated in a\n",
    "    rolling fashion and if it exceeds an interval of thresholds\n",
    "    (defaulting to [0.5, 3.0]) then a long/short signal pair are generated\n",
    "    (for the high threshold) or an exit signal pair are generated (for the\n",
    "    low threshold).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "    self, bars, events, ols_window=100,\n",
    "    zscore_low=0.5, zscore_high=3.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the stat arb strategy.\n",
    "        Parameters:\n",
    "        bars - The DataHandler object that provides bar information\n",
    "        events - The Event Queue object.\n",
    "        \"\"\"\n",
    "        self.bars = bars\n",
    "        self.symbol_list = self.bars.symbol_list\n",
    "        self.events = events\n",
    "        self.ols_window = ols_window\n",
    "        self.zscore_low = zscore_low\n",
    "        self.zscore_high = zscore_high\n",
    "        self.pair = (’AREX’, ’WLL’)\n",
    "        self.datetime = datetime.datetime.utcnow()\n",
    "        self.long_market = False\n",
    "        self.short_market = False\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.io.parsers.read_csv(\n",
    "    \"equity.csv\", header=0,\n",
    "    parse_dates=True, index_col=0\n",
    "    ).sort()\n",
    "    # Plot three charts: Equity curve,\n",
    "    # period returns, drawdowns\n",
    "    fig = plt.figure()\n",
    "    # Set the outer colour to white\n",
    "    fig.patch.set_facecolor(’white’)\n",
    "    # Plot the equity curve\n",
    "    ax1 = fig.add_subplot(311, ylabel=’Portfolio value, %’)\n",
    "    data[’equity_curve’].plot(ax=ax1, color=\"blue\", lw=2.)\n",
    "    plt.grid(True)\n",
    "    # Plot the returns\n",
    "    ax2 = fig.add_subplot(312, ylabel=’Period returns, %’)\n",
    "    data[’returns’].plot(ax=ax2, color=\"black\", lw=2.)\n",
    "    plt.grid(True)\n",
    "    # Plot the returns\n",
    "    ax3 = fig.add_subplot(313, ylabel=’Drawdowns, %’)\n",
    "    data[’drawdown’].plot(ax=ax3, color=\"red\", lw=2.)\n",
    "    plt.grid(True)\n",
    "    # Plot the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Strategy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are going to describe optimisation methods to improve the performance\n",
    "of our trading strategies by tuning the parameters in a systematic fashion. For this we will use\n",
    "mechanisms from the statistical field of Model Selection, such as cross-validation and grid search.\n",
    "\n",
    "The biggest danger when considering parameter optimisation is that of overfitting a model\n",
    "or trading strategy. This problem occurs when a model is trained on an in sample retained slice\n",
    "of training data and is optimised to perform well (by the appropriate performance measure), but\n",
    "performance degrades substantially when applied to out of sample data. For instance, a trading\n",
    "strategy could perform extremely well in the backtest (the in sample data) but when deployed\n",
    "for live trading can be completely unprofitable.\n",
    "\n",
    "In addition to parameters there are numerous means of evaluating the performance of a\n",
    "statistical model and the trading strategy based upon it. We have defined concepts such as the\n",
    "hit rate and the confusion matrix. In addition there are more statistical measures such as the\n",
    "Mean Squared Error (MSE). These are performance measures that would be optimised at the\n",
    "statistical model level, via parameters relevant to their domain.\n",
    "\n",
    "The actual trading strategy is evaluated on different criteria, such as compound annual growth\n",
    "rate (CAGR) and maximum drawdown. We would need to vary entry and exit criteria, as well\n",
    "as other thresholds that are not directly related to the statistical model. Hence this motivates\n",
    "the question as to which set of parameters to optimise and when."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation \n",
    "It is a technique used to assess how a statistical model will generalise to new data\n",
    "that it has not been exposed to before. Such a technique is usually used on predictive models,\n",
    "such as the aforementioned supervised classifiers used to predict the sign of the following daily\n",
    "returns of an asset price series. Fundamentally, the goal of cross validation is to minimise error\n",
    "on out of sample data without leading to an overfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest example of cross validation is known as a training/test split, or a 2-fold cross\n",
    "validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from create_lagged_series import create_lagged_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..\n",
    "# The test data is split into two parts: Before and after 1st Jan 2005.\n",
    "start_test = datetime.datetime(2005,1,1)\n",
    "# Create training and test sets\n",
    "X_train = X[X.index < start_test]\n",
    "X_test = X[X.index >= start_test]\n",
    "y_train = y[y.index < start_test]\n",
    "y_test = y[y.index >= start_test]\n",
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\n",
    "        \"^GSPC\", datetime.datetime(2001,1,10),\n",
    "        datetime.datetime(2005,12,31), lags=5\n",
    "    )\n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.8, random_state=42\n",
    "    )\n",
    "    # Create the (parametrised) models\n",
    "    print(\"Hit Rates/Confusion Matrices:\\n\")\n",
    "    models = [(\"LR\", LogisticRegression()),\n",
    "        (\"LDA\", LDA()),\n",
    "        (\"QDA\", QDA()),\n",
    "        (\"LSVC\", LinearSVC()),\n",
    "        (\"RSVM\", SVC(\n",
    "            C=1000000.0, cache_size=200, class_weight=None,\n",
    "            coef0=0.0, degree=3, gamma=0.0001, kernel=’rbf’,\n",
    "            max_iter=-1, probability=False, random_state=None,\n",
    "            shrinking=True, tol=0.001, verbose=False)\n",
    "        ),\n",
    "        (\"RF\", RandomForestClassifier(\n",
    "            n_estimators=1000, criterion=’gini’,\n",
    "            max_depth=None, min_samples_split=2,\n",
    "            min_samples_leaf=1, max_features=’auto’,\n",
    "            bootstrap=True, oob_score=False, n_jobs=1,\n",
    "            random_state=None, verbose=0)\n",
    "        )]\n",
    "    # Iterate through the models\n",
    "    for m in models:\n",
    "        # Train each of the models on the training set\n",
    "        m[1].fit(X_train, y_train)\n",
    "        # Make an array of predictions on the test set\n",
    "        pred = m[1].predict(X_test)\n",
    "        # Output the hit-rate and the confusion matrix for each model\n",
    "        print(\"%s:\\n%0.3f\" % (m[0], m[1].score(X_test, y_test)))\n",
    "        print(\"%s\\n\" % confusion_matrix(pred, y_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__K-Fold Cross Validation__\n",
    "\n",
    "\n",
    "Rather than partitioning the set into a single training and test set, we can use k-fold cross\n",
    "validation to randomly partition the the set into k equally sized subsamples. For each iteration\n",
    "(of which there are k), one of the k subsamples is retained as a test set, while the remaining\n",
    "k − 1 subsamples together form a training set. A statistical model is then trained on each of the\n",
    "k folds and its performance evaluated on its specific k-th test set.\n",
    "\n",
    "The purpose of this is to combine the results of each model into an emsemble by means of\n",
    "averaging the results of the prediction (or otherwise) to produce a single prediction. The main\n",
    "benefit of using k-fold cross validation is that the every predictor within the original data set is\n",
    "used both for training and testing only once.\n",
    "\n",
    "This motivates a question as to how to choose k, which is now another parameter! Generally,\n",
    "k = 10 is used but one can also perform another analysis to choose an optimal value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from create_lagged_series import create_lagged_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\n",
    "        \"^GSPC\", datetime.datetime(2001,1,10),\n",
    "        datetime.datetime(2005,12,31), lags=5\n",
    "    )\n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    # Create a k-fold cross validation object\n",
    "    kf = cross_validation.KFold(\n",
    "        len(snpret), n_folds=10, indices=False,\n",
    "        shuffle=True, random_state=42\n",
    "    )\n",
    "    # Use the kf object to create index arrays that\n",
    "    # state which elements have been retained for training\n",
    "    # and which elements have beenr retained for testing\n",
    "    # for each k-element iteration\n",
    "    for train_index, test_index in kf:\n",
    "        X_train = X.ix[X.index[train_index]]\n",
    "        X_test = X.ix[X.index[test_index]]\n",
    "        y_train = y.ix[y.index[train_index]]\n",
    "        y_test = y.ix[y.index[test_index]]\n",
    "        # In this instance only use the\n",
    "        # Radial Support Vector Machine (SVM)\n",
    "        print(\"Hit Rate/Confusion Matrix:\")\n",
    "        model = SVC(\n",
    "            C=1000000.0, cache_size=200, class_weight=None,\n",
    "            coef0=0.0, degree=3, gamma=0.0001, kernel=’rbf’,\n",
    "            max_iter=-1, probability=False, random_state=None,\n",
    "            shrinking=True, tol=0.001, verbose=False\n",
    "        )\n",
    "        # Train the model on the retained training data\n",
    "        model.fit(X_train, y_train)\n",
    "        # Make an array of predictions on the test set\n",
    "        pred = model.predict(X_test)\n",
    "        # Output the hit-rate and the confusion matrix for each model\n",
    "        print(\"%0.3f\" % model.score(X_test, y_test))\n",
    "        print(\"%s\\n\" % confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "We have so far seen that k-fold cross validation helps us to avoid overfitting in the data by\n",
    "performing validation on every element of the sample. We now turn our attention to optimising\n",
    "the hyper-parameters of a particular statistical model. Such parameters are those not directly\n",
    "learnt by the model estimation procedure. For instance, C and γ for a support vector machine.\n",
    "In essence they are the parameters that we need to specify when calling the initialisation of each\n",
    "statistical model. For this procedure we will use a process known as a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from create_lagged_series import create_lagged_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\n",
    "        \"^GSPC\", datetime.datetime(2001,1,10),\n",
    "        datetime.datetime(2005,12,31), lags=5\n",
    "    )\n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=42\n",
    "    )\n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [\n",
    "        {’kernel’: [’rbf’], ’gamma’: [1e-3, 1e-4], ’C’: [1, 10, 100, 1000]}\n",
    "    ]\n",
    "    # Perform the grid search on the tuned parameters\n",
    "    model = GridSearchCV(SVC(C=1), tuned_parameters, cv=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Optimised parameters found on training set:\")\n",
    "    print(model.best_estimator_, \"\\n\")\n",
    "    print(\"Grid scores calculated on training set:\")\n",
    "    for params, mean_score, scores in model.grid_scores_:\n",
    "        print(\"%0.3f for %r\" % (mean_score, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising Strategies\n",
    "\n",
    "Up until this point we have concentrated on model selection and optimising the underlying\n",
    "statistical model that (might) form the basis of a trading strategy. However, a predictive model\n",
    "and a functioning, profitable algorithmic strategy are two different entities. We now turn our\n",
    "attention to optimising parameters that have a direct effect on profitability and risk metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
