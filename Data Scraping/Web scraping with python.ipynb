{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Legalities and Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal world, web scraping would not be necessary and each website would provide an API to share their data in a structured format.\n",
    "\n",
    "Is web scraping legal?\n",
    "If the data is going to be republished, then the type of data scraped is important.\n",
    "These cases suggest that when the scraped data constitutes facts (such as business locations and telephone listings), it can be republished. However, if the data is original (such as opinions and reviews), it most likely cannot be republished for copyright reasons.\n",
    "\n",
    "There are three basic types of Intellectual Property: trade‐\n",
    "marks (indicated by a ™ or ® symbol), copyrights (the ubiquitous ©), and patents\n",
    "(sometimes indicated by text noting that the invention is patent protected or a patent\n",
    "number, but often by nothing at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background research\n",
    "Before diving into crawling a website, we should develop an understanding about the scale and structure of our target website.\n",
    "1. Checking robots.txt: for example http://example.webscraping.com/robots.txt\n",
    "2. Examining the Sitemap: Sitemap files are provided by websites to help crawlers locate their updated content\n",
    "without needing to crawl every web page.\n",
    "3. Estimating the size of a website: A quick way to estimate the size of a website is to check the results of Google's\n",
    "crawler (site:example.webscraping.com)\n",
    "4. Identifying the technology used by a website:The type of technology used to build a website will effect how we crawl it. A useful tool to check the kind of technologies a website is built with is the builtwith module:\n",
    "\n",
    "        import builtwith\n",
    "    \n",
    "        builtwith.parse('http://example.webscraping.com')\n",
    "    \n",
    "    \n",
    "5. Finding the owner of a website: For some websites it may matter to us who is the owner. For example, if the owner is known to block web crawlers then it would be wise to be more conservative in our download rate\n",
    "\n",
    "        import whois\n",
    "    \n",
    "        print(whois.whois('appspot.com'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Crawling a website\n",
    "In order to scrape a website, we first need to download its web pages containing the data of interest—a process known as crawling. Three common approaches to crawling a website:\n",
    "1. Crawling a sitemap\n",
    "2. Iterating the database IDs of each web page\n",
    "3. Following web page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a web page\n",
    "\n",
    "def download(url, retriesNum=2, user_agent='WebCrawler'):\n",
    "    \n",
    "    import requests\n",
    "    from requests.exceptions import HTTPError\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    \n",
    "    adapter = HTTPAdapter(max_retries=retriesNum)\n",
    "    session = requests.Session()\n",
    "    session.mount(url, adapter)\n",
    "    headers = {'User-agent': user_agent}\n",
    "    \n",
    "    print('Downloading: ', url)\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "    except ConnectionError as ce:\n",
    "        print(ce)\n",
    "    else:\n",
    "        print('Success!')\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Sitemap crawler\n",
    "\n",
    "def crawl_sitemap(url):\n",
    "    import re \n",
    "    sitmap = download(url) \n",
    "    links = re.findall('<loc>(.*?)</loc>', sitemap) # extract the sitemap links\n",
    "    for link in links: # download each link\n",
    "        html = download(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.ID iteration crawler\n",
    "\n",
    "import itertools\n",
    "\n",
    "max_errors = 5 # maximum number of consecutive download errors allowed\n",
    "num_errors = 0 # current number of consecutive download errors\n",
    "\n",
    "for page in itertools.count(1):\n",
    "    url = 'http://example.webscraping.com/view/-%d' % page\n",
    "    html = download(url)\n",
    "    if html is None:\n",
    "        num_errors += 1\n",
    "        if num_errors == max_errors:\n",
    "            break\n",
    "    else:\n",
    "        num_errors = 0 # success: can scrape the result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Link crawler\n",
    "'''\n",
    "So far, we have implemented two simple crawlers that take advantage of the\n",
    "structure of our sample website to download all the countries. These techniques\n",
    "should be used when available, because they minimize the required amount of web\n",
    "pages to download. However, for other websites, we need to make our crawler act\n",
    "more like a typical user and follow links to reach the content of interest.\n",
    "'''\n",
    "\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "def link_crawler(seed_url, link_regex):\n",
    "    crawl_queue = [seed_url]\n",
    "    seen = set(crawl_queue) # keep track which URL's have seen before\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        # filter for links matching our regular expression\n",
    "        for link in get_links(html):\n",
    "            if re.match(link_regex, link):\n",
    "                link = urllib.parse.urljoin(seed_url, link)\n",
    "                if link not in seen:\n",
    "                    seen.add(link)\n",
    "                    crawl_queue.append(link)\n",
    "            \n",
    "def get_links(html):\n",
    "    # Return a list of links from html\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']',re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_crawler('http://example.webscraping.com', 'example.webscraping.com/(index|view)/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we crawl a website too fast, we risk being blocked or overloading the server.\n",
    "# To minimize these risks, we can throttle our crawl by waiting for a delay between downloads.\n",
    "\n",
    "class Throttle:\n",
    "    \n",
    "    def __init__(self, delay):\n",
    "        # amount of delay between downloads for each domain\n",
    "        self.delay = delay\n",
    "        # timestamp of when a domain was last accessed\n",
    "        self.domains = {}\n",
    "    def wait(self, url):\n",
    "        domain = urlparse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.datetime.now() - last_accessed).seconds\n",
    "        if sleep_secs > 0:\n",
    "            # domain has been accessed recently so need to sleep\n",
    "            time.sleep(sleep_secs)\n",
    "        # update the last accessed time\n",
    "        self.domains[domain] = datetime.datetime.now()\n",
    "        \n",
    "''' We can add throttling to the crawler by calling throttle before every download:\n",
    "throttle = Throttle(delay)\n",
    "throttle.wait(url)\n",
    "result = download(url, headers, proxy=proxy, num_retries=num_retries)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Beautiful Soup & RegEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to make this crawler achieve something by extracting data from each web page, which is known as scraping.\n",
    "We will walk through three approaches to extract data from a web page using regular expressions, Beautiful Soup and lxml. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a*: it can be zero a or 1 or more as like aaa.\n",
    "2. a+: it can be 1 or more as like aaaaa.\n",
    "3. []: matches any character in the brackets like [A-Z]* can be APPLE.\n",
    "4. (): everything in the prantheses are evaluated first.\n",
    "5. {m,n}: Matches between m and n times like a{2,3}b = aab or aaab.\n",
    "6. [^]: Matches any single character that is not in the brackets like [^A-Z]* = apple\n",
    "7. | : or. for example: b(a|e)d = bad or bed\n",
    "8. . :Matches any single character (including symbols, numbers, a space, etc.) like c.r = car or c#r\n",
    "9. ^ : Indicates that a character or subexpression occurs at the beginning of a string. ^a = apple or a\n",
    "10. \\ :An escape character (this allows you to use special characters as their literal meanings).\n",
    "11. dollar sign :Often used at the end of a regular expression, it means “match this up to the end of the string.” Without it, every regular expression has a de facto “.*” at the end of it, accepting strings where only the first part of the string matches. This can be thought of as analogous to the ^ symbol.  [A-Z]*[a-z]*$ = APall\n",
    "12. ?!: “Does not contain.” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " email address:\n",
    " \n",
    "    [A-Za-z0-9\\._+]+@[A-Za-z]+\\.(com|org|edu|net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "html = download(url)\n",
    "re.findall('<td class=\"w2p_fw\">(.*?)</td>', html)\n",
    "\n",
    "# Consider if this table is changed so that the population data is no longer available in the second row.\n",
    "re.findall('<tr id=\"places_area__row\"><td\n",
    "class=\"w2p_fl\"><label for=\"places_area\"\n",
    "id=\"places_area__label\">Area: </label></td><td\n",
    "class=\"w2p_fw\">(.*?)</td>', html)\n",
    "           \n",
    "# There are many other ways the web page could\n",
    "be updated in a way that still breaks the regular expression.            \n",
    "re.findall('<tr id=\"places_area__row\">.*?<td\\s*class=[\"\\']w2p_fw[\"\\']>(.*?)</td>', html)\n",
    "           \n",
    "# From this example, it is clear that regular expressions provide a quick way to scrape data \n",
    "# but are too brittle and will easily break when a web page is updated.\n",
    "# Fortunately, there are better solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "source = requests.get('http://www.tsetmc.com/Loader.aspx?ParTree=151311&i=63917421733088077').text\n",
    "soup = BeautifulSoup(source, 'html.parser')# Other kinds of parsers: lxml, html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle errors\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle('http://www.pythonscraping.com/pages/page1.html')\n",
    "if title == None:\n",
    "    print('Title could not be found')\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four objects in this library:\n",
    "1. BeautifulSoup objects: Instances seen in previous code examples as the variable bs\n",
    "2. Tag objects: Retrieved in lists, or retrieved individually by calling find and find_all on a BeautifulSoup object\n",
    "3. NavigableString objects: Used to represent text within tags, rather than the tags themselves\n",
    "4. Comment objects: Used to find HTML comments in comment tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find and find_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Buy WSwP Directly from O'Reilly:\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Navigation\n",
      "\n",
      "Blog\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "source = requests.get('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(source.text, 'html.parser')\n",
    "# soup.find_all(tagName, tagAttributes) \n",
    "nameList = bs.findAll('div', {'class':'five columns'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all(tag, attributes, recursive, text, limit, keywords)\n",
    "# find(tag, attributes, recursive, text, keywords)\n",
    "\n",
    "# tag:\n",
    "bs.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "\n",
    "# attributes:\n",
    "bs.find_all('span', {'class':{'green', 'red'}})\n",
    "\n",
    "# recursive: The recursive argument is a boolean. How deeply into the document do you want to\n",
    "# go? If recursive is set to True, the find_all function looks into children, and children’s children, \n",
    "# for tags that match your parameters. If it is False, it will look only at the top-level tags in your document. \n",
    "\n",
    "# limit: The limit argument, of course, is used only in the find_all method; find is equivalent \n",
    "# to the same find_all call, with a limit of 1. You might set this if you’re interested\n",
    "# only in retrieving the first x items from the page\n",
    "\n",
    "# keywords: This argument allows you to select tags that contain a particular attribute or set of attributes.\n",
    "bs.find_all(id='title', class_='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating Trees\n",
    "The find_all function is responsible for finding tags based on their name and\n",
    "attributes. But what if you need to find a tag based on its location in a document?\n",
    "That’s where tree navigation comes in handy. You looked at navigating a\n",
    "BeautifulSoup tree in a single direction:\n",
    "\n",
    "        bs.tag.subTag.anotherSubTag\n",
    "\n",
    "Now let’s look at navigating up, across, and diagonally through HTML trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to find only descendants that are children, you can use the .children tag:\n",
    "for child in bs.find('table',{'id':'giftList'}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The BeautifulSoup next_siblings() function makes it trivial to collect data from tables, \n",
    "#especially ones with title rows:\n",
    "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Occasionally, however, you can find yourself in odd situations \n",
    "#that require BeautifulSoup’s parent-finding functions, .parent and .parents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Attributes\n",
    "However, often in web scraping you’re not looking for the content of a tag; you’re\n",
    "looking for its attributes. This becomes especially useful for tags such as a, where the\n",
    "URL it is pointing to is contained within the href attribute; or the img tag, where the\n",
    "target image is contained within the src attribute.\n",
    "\n",
    "    myImgTag.attrs['src']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lxml is a Python wrapper which helps make it faster than Beautiful Soup but also harder to install on some\n",
    "computers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Caching Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support caching, the download function developed in Chapter 1, needs to be modified to check the cache before downloading a URL. We also need to move throttling inside this function and only throttle when a download is made, and not when loading from a cache. To avoid the need to pass various parameters for every download, we will take this opportunity to refactor the download function into a class, so that parameters can be set once in the constructor and reused multiple times. Here is the updated implementation to support this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downloader:\n",
    "    def __init__(self, delay=5, user_agent='WebCrawler', proxies=None, num_retries=1, cache=None):\n",
    "        self.throttle = Throttle(delay)\n",
    "        self.user_agent = user_agent\n",
    "        self.proxies = proxies\n",
    "        self.num_retries = num_retries\n",
    "        self.cache = cache\n",
    "    \n",
    "    # the cache is checked before downloading\n",
    "    def __call__(self, url):\n",
    "        result = None\n",
    "        if self.cache:\n",
    "            try:\n",
    "                result = self.cache[url]\n",
    "            except KeyError:\n",
    "                # url is not available in cache\n",
    "                pass\n",
    "            else:\n",
    "                if self.num_retries > 0 and 500 <= result['code'] < 600:\n",
    "                # server error so ignore result from cache and re-download\n",
    "                    result = None\n",
    "        if result is None:\n",
    "            # result was not loaded from cache so still need to download\n",
    "            self.throttle.wait(url)\n",
    "            proxy = random.choice(self.proxies) if self.proxies\n",
    "                else None\n",
    "            headers = {'User-agent': self.user_agent}\n",
    "            result = self.download(url, headers, proxy, self.num_retries)\n",
    "            if self.cache:\n",
    "                # save result to cache\n",
    "                self.cache[url] = result\n",
    "        return result['html']\n",
    "    \n",
    "    def download(self, url, headers, proxy, num_retries, data=None):\n",
    "        ''' The download method of this class is the same as the previous download function, except now it\n",
    "        returns the HTTP status code along with the downloaded HTML so that error codes \n",
    "        can be stored in the cache.'''\n",
    "        return {'html': html, 'code': code}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link crawler also needs to be slightly updated to support caching by adding the\n",
    "cache parameter, removing the throttle, and replacing the download function with\n",
    "the new class, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_crawler(..., cache=None):\n",
    "    crawl_queue = [seed_url]\n",
    "    seen = {seed_url: 0}\n",
    "    num_urls = 0\n",
    "    rp = get_robots(seed_url)\n",
    "    D = Downloader(delay=delay, user_agent=user_agent, proxies=proxies, num_retries=num_retries, cache=cache)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        depth = seen[url]\n",
    "        # check url passes robots.txt restrictions\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            html = D(url)\n",
    "            links = []\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cache downloads, we will first try the obvious solution and save web pages to the filesystem. To do this, we will need a way to map URLs to a safe cross-platform filename (according to the limitations in different OS using different filesystem). To keep our file path safe across these filesystems, it needs to be restricted to numbers, letters, basic punctuation, and replace all other characters with an underscore, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http_//example.webscraping.com/default/view/Australia-1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "url = 'http://example.webscraping.com/default/view/Australia-1'\n",
    "re.sub('[^/0-9a-zA-Z\\-.,;_ ]', '_', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the filename and the parent directories need to be restricted to 255 characters to meet the length limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/'.join(segment[:255] for segment in filename.split('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an edge case that needs to be considered, where the URL path ends\n",
    "with a slash (/), and the empty string after this slash would be an invalid filename.\n",
    "However, removing this slash to use the parent for the filename would prevent\n",
    "saving other URLs. The solution our disk cache will use is appending index.html to the filename\n",
    "when the URL path ends with a slash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlparse\n",
    "components = urlparse.urlsplit('http://example.webscraping.com/index/')\n",
    "path = components.path\n",
    "if not path:\n",
    "    path = '/index.html'\n",
    "elif path.endswith('/'):\n",
    "    path += 'index.html'\n",
    "filename = components.netloc + path + components.query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, using this logic to map a URL to a\n",
    "filename will form the main part of the disk cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urlparse\n",
    "import pickle #  The pickle module is used to convert the input to a string, which is then saved to disk.\n",
    "\n",
    "class DiskCache:\n",
    "    def __init__(self, cache_dir='cache'):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_length = max_length\n",
    "    def url_to_path(self, url):\n",
    "        '''Create file system path for this URL\n",
    "        '''\n",
    "        components = urlparse.urlsplit(url)\n",
    "        # append index.html to empty paths\n",
    "        path = components.path\n",
    "        if not path:\n",
    "            path = '/index.html'\n",
    "        elif path.endswith('/'):\n",
    "            path += 'index.html'\n",
    "        filename = components.netloc + path + components.query\n",
    "        # replace invalid characters\n",
    "        filename = re.sub('[^/0-9a-zA-Z\\-.,;_ ]', '_', filename)\n",
    "        # restrict maximum number of characters\n",
    "        filename = '/'.join(segment[:250] for segment in\n",
    "            filename.split('/'))\n",
    "    return os.path.join(self.cache_dir, filename)\n",
    "    def __getitem__(self, url):\n",
    "        \"\"\"Load data from disk for this URL\n",
    "        \"\"\"\n",
    "        path = self.url_to_path(url)\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as fp:\n",
    "                return pickle.load(fp)\n",
    "        else:\n",
    "            # URL has not yet been cached\n",
    "            raise KeyError(url + ' does not exist')\n",
    "    def __setitem__(self, url, result):\n",
    "        \"\"\"Save data to disk for this url\n",
    "        \"\"\"\n",
    "        path = self.url_to_path(url)\n",
    "        folder = os.path.dirname(path)\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        with open(path, 'wb') as fp:\n",
    "            fp.write(pickle.dumps(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawbacks\n",
    "# we applied various restrictions to map the URL to a safe filename, \n",
    "# but an unfortunate consequence of this is that some URLs will map to the same filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NoSQL stands for Not Only SQL and is a relatively new approach to database design.\n",
    "The traditional relational model used a fixed schema and splits the data into\n",
    "tables. However, with large datasets, the data is too big for a single server and needs\n",
    "to be scaled across multiple servers. This does not fit well with the relational model\n",
    "because, when querying multiple tables, the data will not necessarily be available on\n",
    "the same server. NoSQL databases, on the other hand, are generally schemaless and\n",
    "designed from the start to shard seamlessly across servers. There have been multiple\n",
    "approaches to achieve this that fit under the NoSQL umbrella. There are column data\n",
    "stores, such as HBase; key-value stores, such as Redis; document-oriented databases,\n",
    "such as MongoDB; and graph databases, such as Neo4j.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Concurrent Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Dynamic Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the United Nations Global Audit of Web Accessibility, 73 percent of leading websites rely on JavaScript for important functionalities (client-side language that’s ubiquitous in modern web pages: JavaScript).It can be used to collect information for user tracking,\n",
    "submit forms without reloading the page, embed multimedia, and even power entire\n",
    "online games. \n",
    "\n",
    "    <script>\n",
    "    alert(\"This creates a pop-up using JavaScript\");\n",
    "    </script>\n",
    "\n",
    "\n",
    "The consequence of this is that for many web pages the content that is displayed in our web browser is not available in the original HTML, and the scraping techniques covered so far will not work. There are two approaches to scraping data from dynamic JavaScript dependent websites:\n",
    "\n",
    "• Reverse engineering JavaScript\n",
    "\n",
    "• Rendering JavaScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJAX stands for Asynchronous JavaScript and XML and was\n",
    "coined in 2005 to describe the features available across web\n",
    "browsers that made dynamic web applications possible.This allowed JavaScript to make\n",
    "HTTP requests to a remote server and receive responses, which\n",
    "meant that a web application could send and receive data. The\n",
    "traditional way to communicate between client and server was\n",
    "to refresh the entire web page, which resulted in a poor user\n",
    "experience and wasted bandwidth when only a small amount of\n",
    "data needed to be transmitted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse engineering a dynamic web page\n",
    "\n",
    "The data is loaded dynamically with JavaScript. To scrape this data, we need to understand how the web\n",
    "page loads this data, a process known as reverse engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "template_url = 'http://example.webscraping.com/ajax/search.json?page={}&page_size=10&search_term={}'\n",
    "countries = set()\n",
    "for letter in string.lowercase:\n",
    "    page = 0\n",
    "    while True:\n",
    "        html = D(template_url.format(page, letter))\n",
    "        try:\n",
    "            ajax = json.loads(html)\n",
    "        except ValueError as e:\n",
    "            print e\n",
    "            ajax = None\n",
    "        else:\n",
    "            for record in ajax['records']:\n",
    "                countries.add(record['country'])\n",
    "        page += 1\n",
    "        if ajax is None or page >= ajax['num_pages']:\n",
    "            break\n",
    "open('countries.txt', 'w').write('\\n'.join(sorted(countries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering a dynamic web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some websites will be very complex and difficult to understand,\n",
    "even with a tool like Firebug. For example, if the website has been built with Google\n",
    "Web Toolkit (GWT), the resulting JavaScript code will be machine-generated and\n",
    "minified. This generated JavaScript code can be cleaned with a tool such as JS\n",
    "beautifier, but the result will be verbose and the original variable names will be\n",
    "lost, so it is difficult to work with. With enough effort, any website can be reverse\n",
    "engineered. However, this effort can be avoided by instead using a browser rendering\n",
    "engine, which is the part of the web browser that parses HTML, applies the CSS\n",
    "formatting, and executes JavaScript to display a web page as we expect. In this\n",
    "section, the WebKit rendering engine will be used, which has a convenient Python\n",
    "interface through the Qt framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyQt or PySide\n",
    "# The following snippet can be used to import whichever Qt binding is installed:\n",
    "try:\n",
    "    from PySide.QtGui import *\n",
    "    from PySide.QtCore import *\n",
    "    from PySide.QtWebKit import *\n",
    "except ImportError:\n",
    "    from PyQt4.QtGui import *\n",
    "    from PyQt4.QtCore import *\n",
    "    from PyQt4.QtWebKit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJAX search\n",
    "\n",
    "# This instantiates the QApplication object that the Qt framework requires to be created before other Qt objects to perform various initializations.\n",
    "app = QApplication([])\n",
    "# Next, a QWebView object is created, which is a container for the web documents.\n",
    "webview = QWebView()\n",
    "# A QEventLoop object is created, which will be used to create a local event loop\n",
    "loop = QEventLoop()\n",
    "# The loadFinished callback of the QwebView object is connected to the quit method of QEventLoop \n",
    "# so that when a web page finishes loading, the event loop will be stopped. \n",
    "# The URL to load is then passed to QWebView. \n",
    "# PyQt requires that this URL string is wrapped by a QUrl object, while for PySide, this is optional.\n",
    "webview.loadFinished.connect(loop.quit)\n",
    "# The QWebView load method is asynchronous, so execution will immediately pass to the next line \n",
    "# while the web page is loading—however, we want to wait until this web page is loaded, so loop.exec_() is called to start the event loop.\n",
    "webview.load(QUrl(url))\n",
    "# When the web page completes loading, the event loop will exit and execution can move to the next line, \n",
    "# where the resulting HTML of this loaded web page is extracted.\n",
    "loop.exec_()\n",
    "# Next, the QWebView GUI show() method is called so that the render window is displayed, which is useful for debugging. \n",
    "webview.show()\n",
    "# Then, a reference to the frame is created to make the following lines shorter.\n",
    "frame = webview.page().mainFrame()\n",
    "# The QWebFrame class has many useful methods to interact with a web page. \n",
    "# The following two lines use the CSS patterns to locate an element in the frame, and then set the search parameters.\n",
    "frame.findFirstElement('#search_term').\n",
    "setAttribute('value', '.')\n",
    "frame.findFirstElement('#page_size option:checked').\n",
    "setPlainText('1000')\n",
    "frame.findFirstElement('#search').\n",
    "# Then, the form is submitted with the evaluateJavaScript() method to simulate a click event.\n",
    "evaluateJavaScript('this.click()')\n",
    "app.exec_()\n",
    "html = webview.page().mainFrame().toHtml()\n",
    "tree = lxml.html.fromstring(html)\n",
    "tree.cssselect('#result')[0].text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of implementing our WebKit crawler is scraping the search results,\n",
    "which turns out to be the most difficult part because it is not obvious when the AJAX\n",
    "event is complete and the country data is ready. There are three possible approaches\n",
    "to deal with this:\n",
    "\n",
    "• Wait a set amount of time and hope that the AJAX event is complete by then\n",
    "\n",
    "• Override Qt's network manager to track when the URL requests are complete\n",
    "\n",
    "• Poll the web page for the expected content to appear\n",
    "\n",
    "The first option is the simplest to implement but is inefficient, because if a safe timeout\n",
    "is set, then usually a lot more time is spent waiting than necessary. Also, when the\n",
    "network is slower than usual, a fixed timeout could fail. The second option is more\n",
    "efficient but cannot be applied when the delay is from the client side rather than server\n",
    "side—for example, if the download is complete, but a button needs to be pressed\n",
    "before the content is displayed. The third option is more reliable and straightforward to\n",
    "implement, though there is the minor drawback of wasting CPU cycles when checking\n",
    "whether the content has loaded yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium\n",
    "\n",
    "With the WebKit library used in the preceding example, we have full control to\n",
    "customize the browser renderer to behave as we need it to. If this level of flexibility\n",
    "is not needed, a good alternative is Selenium, which provides an API to automate the\n",
    "web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox() # When this command is run, an empty browser window will pop up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load a web page in the chosen web browser, the get() method is called:\n",
    "driver.get('http://example.webscraping.com/search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set which element to select, the ID of the search textbox can be used.\n",
    "driver.find_element_by_id('search_term').send_keys('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To return all results in a single search, we want to set the page size to 1000. \n",
    "# However, this is not straightforward because Selenium is designed to interact with the browser,\n",
    "# rather than to modify the web page content. \n",
    "# To get around this limitation, we can use JavaScript to set the select box content:\n",
    "js = \"document.getElementById('page_size').options[1].text = '1000'\"\n",
    "driver.execute_script(js);\n",
    "driver.find_element_by_id('search').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to wait for the AJAX request to complete before loading the results, which was the hardest part of the script in the previous WebKit implementation.\n",
    "# Fortunately, Selenium provides a simple solution to this problem by setting a timeout with the implicitly_wait() method:\n",
    "driver.implicitly_wait(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = driver.find_elements_by_css_selector('#results a')\n",
    "countries = [link.text for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Interacting With Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Solving CAPTCHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy is a popular web scraping framework that comes with many high-level\n",
    "functions to make scraping websites easier. We will cover Portia, which is an application based on Scrapy that\n",
    "allows you to scrape a website through a point and click interface.\n",
    "\n",
    "We will use the following commands in this chapter:\n",
    "\n",
    "• startproject: Creates a new project\n",
    "\n",
    "• genspider: Generates a new spider from a template\n",
    "\n",
    "• crawl: Runs a spider\n",
    "\n",
    "• shell: Starts the interactive scraping console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
